{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0+cu118\n",
            "True\n",
            "Package    Version    Source\n",
            "---------  ---------  ---------------------------------------------------\n",
            "mmcv       2.2.0      https://github.com/open-mmlab/mmcv\n",
            "mmdet      3.3.0      d:\\codes\\python\\ai\\4_baseline\\openmmlab\\mmdetection\n",
            "mmengine   0.10.4     https://github.com/open-mmlab/mmengine\n",
            "mmyolo     0.6.0      d:\\codes\\python\\ai\\4_baseline\\openmmlab\\mmyolo\n"
          ]
        }
      ],
      "source": [
        "# 检查 PyTorch 版本\n",
        "import torch, torchvision\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# 查看安装结果\n",
        "!mim list\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 模型分析步骤  \n",
        "完整步骤如下：  \n",
        "数据集准备：tools/misc/download_dataset.py  \n",
        "使用 labelme 和算法进行辅助标注：demo/image_demo.py + labelme  \n",
        "使用脚本转换成 COCO 数据集格式：tools/dataset_converters/labelme2coco.py  \n",
        "数据集划分为训练集、验证集和测试集：tools/misc/coco_split.py  \n",
        "构建 config 文件 : python tools/misc/print_config.py /PATH/TO/CONFIG  \n",
        "数据集可视化分析：tools/analysis_tools/dataset_analysis.py configs/my/yolov8_swin_rip.py  \n",
        "优化 anchor 尺寸：tools/analysis_tools/optimize_anchors.py configs/my/yolov8_swin_rip.py   \n",
        "可视化数据处理部分：tools/analysis_tools/browse_dataset.py configs/my/yolov8_swin_rip.py  \n",
        "启动训练：tools/train.py  \n",
        "模型推理：demo/image_demo.py  \n",
        "模型部署  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 常用命令\n",
        "### 下载权重+推理验证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 步骤 1. 我们需要下载配置文件和模型权重文件。\n",
        "! mim download mmyolo --config yolov5_s-v61_syncbn_fast_8xb16-300e_coco --dest .\n",
        "\n",
        "# 步骤 2. 推理验证(源码安装)\n",
        "! python demo/image_demo.py demo/demo.jpg \\\n",
        "                          yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py \\\n",
        "                          yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth\n",
        "\n",
        "# 可选参数\n",
        "# --out-dir ./output *检测结果输出到指定目录下，默认为./output, 当--show参数存在时，不保存检测结果\n",
        "# --device cuda:0    *使用的计算资源，包括cuda, cpu等，默认为cuda:0\n",
        "# --show             *使用该参数表示在屏幕上显示检测结果，默认为False\n",
        "# --score-thr 0.3    *置信度阈值，默认为0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 步骤 2. 推理验证(MIM安装)\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "config_file = 'yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py'\n",
        "checkpoint_file = 'yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth'\n",
        "model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
        "inference_detector(model, 'demo/demo.jpg')\n",
        "# 你将会看到一个包含 DetDataSample 的列表，预测结果在 pred_instance 里，包含有预测框、预测分数 和 预测类别。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Yolo V8 + Swin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 启动训练\n",
        "!python tools/train.py configs/yolov8/yolov8_s_swin_t-v61_1xb2-1e_coco128.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Yolo V5 + Swin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 启动训练\n",
        "!python tools/train.py configs/yolov5/yolov5_s_swin_t-v61_4xb4-30e_rip.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 更换网络组件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'timm'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 使用第三方网络模块\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 安装 timm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# %pip install timm\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 查看 timm 中支持的模型\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model_names \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mlist_models(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum of models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(model_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
          ]
        }
      ],
      "source": [
        "# 使用第三方网络模块\n",
        "# 安装 timm\n",
        "# %pip install timm\n",
        "\n",
        "# 查看 timm 中支持的模型\n",
        "import timm\n",
        "model_names = timm.list_models(pretrained=True)\n",
        "print(f'num of models: {len(model_names)}')\n",
        "print(model_names)\n",
        "\n",
        "# 示例\n",
        "# 如果想将 timm 中 `mobilevitv2_050` 作为 `YOLOv5` 的主干网络，则配置文件如下：\n",
        "# 导入 mmcls.models 使得可以调用 mmcls 中注册的模块\n",
        "custom_imports = dict(imports=['mmcls.models'], allow_failed_imports=False)\n",
        "\n",
        "model = dict(\n",
        "    backbone=dict(\n",
        "        _delete_=True,  # 将 _base_ 中关于 backbone 的字段删除\n",
        "        type='mmcls.TIMMBackbone',  # 使用 mmcls 中的 timm 主干网络\n",
        "        model_name='mobilevitv2_050',  # 使用 TIMM 中的 mobilevitv2_050\n",
        "        # ...\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用自监督\n",
        "\n",
        "# 安装 mmselfsup\n",
        "!mim install \"mmselfsup>=1.0.0rc3\"\n",
        "\n",
        "# import mmselfsup\n",
        "# model_names = mmselfsup.list_models(pretrained=True)\n",
        "# print(f'num of models: {len(model_names)}')\n",
        "# print(model_names)\n",
        "\n",
        "# 导入 mmselfsup.models 使得可以调用 mmselfsup 中注册的模块\n",
        "custom_imports = dict(imports=['mmselfsup.models'], allow_failed_imports=False)\n",
        "\n",
        "model = dict(\n",
        "    backbone=dict(\n",
        "        _delete_=True, # 将 _base_ 中关于 backbone 的字段删除\n",
        "        type='mmselfsup.ResNet',\n",
        "        # ...\n",
        "    ))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 确定主干网络输出通道数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([1, 192, 144, 144]), torch.Size([1, 384, 72, 72]), torch.Size([1, 768, 36, 36])]\n"
          ]
        }
      ],
      "source": [
        "## 3 彩蛋：如何确定主干网络输出通道数\n",
        "# PPYOLO-E 最大模型 x 中的 `widen_factor` 为 1.25。假设我们想要构建一个更大的网络，将  `widen_factor` 设为 1.5，此时其主干网络 `PPYOLOECSPResNet` 的输出通道数会是多少呢？\n",
        "\n",
        "import torch\n",
        "from mmyolo.models import PPYOLOECSPResNet\n",
        "from mmyolo.utils import register_all_modules\n",
        "from mmdet.models.backbones import SwinTransformer\n",
        "# 注册所有模块\n",
        "register_all_modules()\n",
        "\n",
        "imgs = torch.randn(1, 3, 1150, 1150)\n",
        "out_indices=(1, 2, 3)\n",
        "# out_indices=(0,1,2,3)\n",
        "model = SwinTransformer(init_cfg=r'D:\\Codes\\Python\\AI\\6_Model\\swin_v2_b-781e5279.pth', out_indices=out_indices)\n",
        "# model = SwinTransformer(out_indices=out_indices)\n",
        "out = model(imgs)\n",
        "out_shapes = [out[i].shape for i in range(len(out_indices))]\n",
        "print(out_shapes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 打印PTH文件结构"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "backbone.patch_embed.proj.weight  torch.Size([96, 3, 4, 4])\n",
            "backbone.patch_embed.proj.bias  torch.Size([96])\n",
            "backbone.patch_embed.norm.weight  torch.Size([96])\n",
            "backbone.patch_embed.norm.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.0.norm1.weight  torch.Size([96])\n",
            "backbone.layers.0.blocks.0.norm1.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.0.attn.relative_position_bias_table  torch.Size([169, 3])\n",
            "backbone.layers.0.blocks.0.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.0.blocks.0.attn.qkv.weight  torch.Size([288, 96])\n",
            "backbone.layers.0.blocks.0.attn.qkv.bias  torch.Size([288])\n",
            "backbone.layers.0.blocks.0.attn.proj.weight  torch.Size([96, 96])\n",
            "backbone.layers.0.blocks.0.attn.proj.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.0.norm2.weight  torch.Size([96])\n",
            "backbone.layers.0.blocks.0.norm2.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.0.mlp.fc1.weight  torch.Size([384, 96])\n",
            "backbone.layers.0.blocks.0.mlp.fc1.bias  torch.Size([384])\n",
            "backbone.layers.0.blocks.0.mlp.fc2.weight  torch.Size([96, 384])\n",
            "backbone.layers.0.blocks.0.mlp.fc2.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.1.norm1.weight  torch.Size([96])\n",
            "backbone.layers.0.blocks.1.norm1.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.1.attn.relative_position_bias_table  torch.Size([169, 3])\n",
            "backbone.layers.0.blocks.1.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.0.blocks.1.attn.qkv.weight  torch.Size([288, 96])\n",
            "backbone.layers.0.blocks.1.attn.qkv.bias  torch.Size([288])\n",
            "backbone.layers.0.blocks.1.attn.proj.weight  torch.Size([96, 96])\n",
            "backbone.layers.0.blocks.1.attn.proj.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.1.norm2.weight  torch.Size([96])\n",
            "backbone.layers.0.blocks.1.norm2.bias  torch.Size([96])\n",
            "backbone.layers.0.blocks.1.mlp.fc1.weight  torch.Size([384, 96])\n",
            "backbone.layers.0.blocks.1.mlp.fc1.bias  torch.Size([384])\n",
            "backbone.layers.0.blocks.1.mlp.fc2.weight  torch.Size([96, 384])\n",
            "backbone.layers.0.blocks.1.mlp.fc2.bias  torch.Size([96])\n",
            "backbone.layers.0.downsample.reduction.weight  torch.Size([192, 384])\n",
            "backbone.layers.0.downsample.norm.weight  torch.Size([384])\n",
            "backbone.layers.0.downsample.norm.bias  torch.Size([384])\n",
            "backbone.layers.1.blocks.0.norm1.weight  torch.Size([192])\n",
            "backbone.layers.1.blocks.0.norm1.bias  torch.Size([192])\n",
            "backbone.layers.1.blocks.0.attn.relative_position_bias_table  torch.Size([169, 6])\n",
            "backbone.layers.1.blocks.0.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.1.blocks.0.attn.qkv.weight  torch.Size([576, 192])\n",
            "backbone.layers.1.blocks.0.attn.qkv.bias  torch.Size([576])\n",
            "backbone.layers.1.blocks.0.attn.proj.weight  torch.Size([192, 192])\n",
            "backbone.layers.1.blocks.0.attn.proj.bias  torch.Size([192])\n",
            "backbone.layers.1.blocks.0.norm2.weight  torch.Size([192])\n",
            "backbone.layers.1.blocks.0.norm2.bias  torch.Size([192])\n",
            "backbone.layers.1.blocks.0.mlp.fc1.weight  torch.Size([768, 192])\n",
            "backbone.layers.1.blocks.0.mlp.fc1.bias  torch.Size([768])\n",
            "backbone.layers.1.blocks.0.mlp.fc2.weight  torch.Size([192, 768])\n",
            "backbone.layers.1.blocks.0.mlp.fc2.bias  torch.Size([192])\n",
            "backbone.layers.1.blocks.1.norm1.weight  torch.Size([192])\n",
            "backbone.layers.1.blocks.1.norm1.bias  torch.Size([192])\n",
            "backbone.layers.1.blocks.1.attn.relative_position_bias_table  torch.Size([169, 6])\n",
            "backbone.layers.1.blocks.1.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.1.blocks.1.attn.qkv.weight  torch.Size([576, 192])\n",
            "backbone.layers.1.blocks.1.attn.qkv.bias  torch.Size([576])\n",
            "backbone.layers.1.blocks.1.attn.proj.weight  torch.Size([192, 192])\n",
            "backbone.layers.1.blocks.1.attn.proj.bias  torch.Size([192])\n",
            "backbone.layers.1.blocks.1.norm2.weight  torch.Size([192])\n",
            "backbone.layers.1.blocks.1.norm2.bias  torch.Size([192])\n",
            "backbone.layers.1.blocks.1.mlp.fc1.weight  torch.Size([768, 192])\n",
            "backbone.layers.1.blocks.1.mlp.fc1.bias  torch.Size([768])\n",
            "backbone.layers.1.blocks.1.mlp.fc2.weight  torch.Size([192, 768])\n",
            "backbone.layers.1.blocks.1.mlp.fc2.bias  torch.Size([192])\n",
            "backbone.layers.1.downsample.reduction.weight  torch.Size([384, 768])\n",
            "backbone.layers.1.downsample.norm.weight  torch.Size([768])\n",
            "backbone.layers.1.downsample.norm.bias  torch.Size([768])\n",
            "backbone.layers.2.blocks.0.norm1.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.0.norm1.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.0.attn.relative_position_bias_table  torch.Size([169, 12])\n",
            "backbone.layers.2.blocks.0.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.2.blocks.0.attn.qkv.weight  torch.Size([1152, 384])\n",
            "backbone.layers.2.blocks.0.attn.qkv.bias  torch.Size([1152])\n",
            "backbone.layers.2.blocks.0.attn.proj.weight  torch.Size([384, 384])\n",
            "backbone.layers.2.blocks.0.attn.proj.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.0.norm2.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.0.norm2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.0.mlp.fc1.weight  torch.Size([1536, 384])\n",
            "backbone.layers.2.blocks.0.mlp.fc1.bias  torch.Size([1536])\n",
            "backbone.layers.2.blocks.0.mlp.fc2.weight  torch.Size([384, 1536])\n",
            "backbone.layers.2.blocks.0.mlp.fc2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.1.norm1.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.1.norm1.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.1.attn.relative_position_bias_table  torch.Size([169, 12])\n",
            "backbone.layers.2.blocks.1.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.2.blocks.1.attn.qkv.weight  torch.Size([1152, 384])\n",
            "backbone.layers.2.blocks.1.attn.qkv.bias  torch.Size([1152])\n",
            "backbone.layers.2.blocks.1.attn.proj.weight  torch.Size([384, 384])\n",
            "backbone.layers.2.blocks.1.attn.proj.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.1.norm2.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.1.norm2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.1.mlp.fc1.weight  torch.Size([1536, 384])\n",
            "backbone.layers.2.blocks.1.mlp.fc1.bias  torch.Size([1536])\n",
            "backbone.layers.2.blocks.1.mlp.fc2.weight  torch.Size([384, 1536])\n",
            "backbone.layers.2.blocks.1.mlp.fc2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.2.norm1.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.2.norm1.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.2.attn.relative_position_bias_table  torch.Size([169, 12])\n",
            "backbone.layers.2.blocks.2.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.2.blocks.2.attn.qkv.weight  torch.Size([1152, 384])\n",
            "backbone.layers.2.blocks.2.attn.qkv.bias  torch.Size([1152])\n",
            "backbone.layers.2.blocks.2.attn.proj.weight  torch.Size([384, 384])\n",
            "backbone.layers.2.blocks.2.attn.proj.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.2.norm2.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.2.norm2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.2.mlp.fc1.weight  torch.Size([1536, 384])\n",
            "backbone.layers.2.blocks.2.mlp.fc1.bias  torch.Size([1536])\n",
            "backbone.layers.2.blocks.2.mlp.fc2.weight  torch.Size([384, 1536])\n",
            "backbone.layers.2.blocks.2.mlp.fc2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.3.norm1.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.3.norm1.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.3.attn.relative_position_bias_table  torch.Size([169, 12])\n",
            "backbone.layers.2.blocks.3.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.2.blocks.3.attn.qkv.weight  torch.Size([1152, 384])\n",
            "backbone.layers.2.blocks.3.attn.qkv.bias  torch.Size([1152])\n",
            "backbone.layers.2.blocks.3.attn.proj.weight  torch.Size([384, 384])\n",
            "backbone.layers.2.blocks.3.attn.proj.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.3.norm2.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.3.norm2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.3.mlp.fc1.weight  torch.Size([1536, 384])\n",
            "backbone.layers.2.blocks.3.mlp.fc1.bias  torch.Size([1536])\n",
            "backbone.layers.2.blocks.3.mlp.fc2.weight  torch.Size([384, 1536])\n",
            "backbone.layers.2.blocks.3.mlp.fc2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.4.norm1.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.4.norm1.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.4.attn.relative_position_bias_table  torch.Size([169, 12])\n",
            "backbone.layers.2.blocks.4.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.2.blocks.4.attn.qkv.weight  torch.Size([1152, 384])\n",
            "backbone.layers.2.blocks.4.attn.qkv.bias  torch.Size([1152])\n",
            "backbone.layers.2.blocks.4.attn.proj.weight  torch.Size([384, 384])\n",
            "backbone.layers.2.blocks.4.attn.proj.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.4.norm2.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.4.norm2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.4.mlp.fc1.weight  torch.Size([1536, 384])\n",
            "backbone.layers.2.blocks.4.mlp.fc1.bias  torch.Size([1536])\n",
            "backbone.layers.2.blocks.4.mlp.fc2.weight  torch.Size([384, 1536])\n",
            "backbone.layers.2.blocks.4.mlp.fc2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.5.norm1.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.5.norm1.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.5.attn.relative_position_bias_table  torch.Size([169, 12])\n",
            "backbone.layers.2.blocks.5.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.2.blocks.5.attn.qkv.weight  torch.Size([1152, 384])\n",
            "backbone.layers.2.blocks.5.attn.qkv.bias  torch.Size([1152])\n",
            "backbone.layers.2.blocks.5.attn.proj.weight  torch.Size([384, 384])\n",
            "backbone.layers.2.blocks.5.attn.proj.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.5.norm2.weight  torch.Size([384])\n",
            "backbone.layers.2.blocks.5.norm2.bias  torch.Size([384])\n",
            "backbone.layers.2.blocks.5.mlp.fc1.weight  torch.Size([1536, 384])\n",
            "backbone.layers.2.blocks.5.mlp.fc1.bias  torch.Size([1536])\n",
            "backbone.layers.2.blocks.5.mlp.fc2.weight  torch.Size([384, 1536])\n",
            "backbone.layers.2.blocks.5.mlp.fc2.bias  torch.Size([384])\n",
            "backbone.layers.2.downsample.reduction.weight  torch.Size([768, 1536])\n",
            "backbone.layers.2.downsample.norm.weight  torch.Size([1536])\n",
            "backbone.layers.2.downsample.norm.bias  torch.Size([1536])\n",
            "backbone.layers.3.blocks.0.norm1.weight  torch.Size([768])\n",
            "backbone.layers.3.blocks.0.norm1.bias  torch.Size([768])\n",
            "backbone.layers.3.blocks.0.attn.relative_position_bias_table  torch.Size([169, 24])\n",
            "backbone.layers.3.blocks.0.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.3.blocks.0.attn.qkv.weight  torch.Size([2304, 768])\n",
            "backbone.layers.3.blocks.0.attn.qkv.bias  torch.Size([2304])\n",
            "backbone.layers.3.blocks.0.attn.proj.weight  torch.Size([768, 768])\n",
            "backbone.layers.3.blocks.0.attn.proj.bias  torch.Size([768])\n",
            "backbone.layers.3.blocks.0.norm2.weight  torch.Size([768])\n",
            "backbone.layers.3.blocks.0.norm2.bias  torch.Size([768])\n",
            "backbone.layers.3.blocks.0.mlp.fc1.weight  torch.Size([3072, 768])\n",
            "backbone.layers.3.blocks.0.mlp.fc1.bias  torch.Size([3072])\n",
            "backbone.layers.3.blocks.0.mlp.fc2.weight  torch.Size([768, 3072])\n",
            "backbone.layers.3.blocks.0.mlp.fc2.bias  torch.Size([768])\n",
            "backbone.layers.3.blocks.1.norm1.weight  torch.Size([768])\n",
            "backbone.layers.3.blocks.1.norm1.bias  torch.Size([768])\n",
            "backbone.layers.3.blocks.1.attn.relative_position_bias_table  torch.Size([169, 24])\n",
            "backbone.layers.3.blocks.1.attn.relative_position_index  torch.Size([49, 49])\n",
            "backbone.layers.3.blocks.1.attn.qkv.weight  torch.Size([2304, 768])\n",
            "backbone.layers.3.blocks.1.attn.qkv.bias  torch.Size([2304])\n",
            "backbone.layers.3.blocks.1.attn.proj.weight  torch.Size([768, 768])\n",
            "backbone.layers.3.blocks.1.attn.proj.bias  torch.Size([768])\n",
            "backbone.layers.3.blocks.1.norm2.weight  torch.Size([768])\n",
            "backbone.layers.3.blocks.1.norm2.bias  torch.Size([768])\n",
            "backbone.layers.3.blocks.1.mlp.fc1.weight  torch.Size([3072, 768])\n",
            "backbone.layers.3.blocks.1.mlp.fc1.bias  torch.Size([3072])\n",
            "backbone.layers.3.blocks.1.mlp.fc2.weight  torch.Size([768, 3072])\n",
            "backbone.layers.3.blocks.1.mlp.fc2.bias  torch.Size([768])\n",
            "backbone.norm0.weight  torch.Size([96])\n",
            "backbone.norm0.bias  torch.Size([96])\n",
            "backbone.norm1.weight  torch.Size([192])\n",
            "backbone.norm1.bias  torch.Size([192])\n",
            "backbone.norm2.weight  torch.Size([384])\n",
            "backbone.norm2.bias  torch.Size([384])\n",
            "backbone.norm3.weight  torch.Size([768])\n",
            "backbone.norm3.bias  torch.Size([768])\n",
            "neck.lateral_convs.0.conv.weight  torch.Size([256, 96, 1, 1])\n",
            "neck.lateral_convs.0.conv.bias  torch.Size([256])\n",
            "neck.lateral_convs.1.conv.weight  torch.Size([256, 192, 1, 1])\n",
            "neck.lateral_convs.1.conv.bias  torch.Size([256])\n",
            "neck.lateral_convs.2.conv.weight  torch.Size([256, 384, 1, 1])\n",
            "neck.lateral_convs.2.conv.bias  torch.Size([256])\n",
            "neck.lateral_convs.3.conv.weight  torch.Size([256, 768, 1, 1])\n",
            "neck.lateral_convs.3.conv.bias  torch.Size([256])\n",
            "neck.fpn_convs.0.conv.weight  torch.Size([256, 256, 3, 3])\n",
            "neck.fpn_convs.0.conv.bias  torch.Size([256])\n",
            "neck.fpn_convs.1.conv.weight  torch.Size([256, 256, 3, 3])\n",
            "neck.fpn_convs.1.conv.bias  torch.Size([256])\n",
            "neck.fpn_convs.2.conv.weight  torch.Size([256, 256, 3, 3])\n",
            "neck.fpn_convs.2.conv.bias  torch.Size([256])\n",
            "neck.fpn_convs.3.conv.weight  torch.Size([256, 256, 3, 3])\n",
            "neck.fpn_convs.3.conv.bias  torch.Size([256])\n",
            "rpn_head.rpn_conv.weight  torch.Size([256, 256, 3, 3])\n",
            "rpn_head.rpn_conv.bias  torch.Size([256])\n",
            "rpn_head.rpn_cls.weight  torch.Size([3, 256, 1, 1])\n",
            "rpn_head.rpn_cls.bias  torch.Size([3])\n",
            "rpn_head.rpn_reg.weight  torch.Size([18, 256, 1, 1])\n",
            "rpn_head.rpn_reg.bias  torch.Size([18])\n",
            "roi_head.bbox_head.fc_cls.weight  torch.Size([16, 1024])\n",
            "roi_head.bbox_head.fc_cls.bias  torch.Size([16])\n",
            "roi_head.bbox_head.fc_reg.weight  torch.Size([5, 1024])\n",
            "roi_head.bbox_head.fc_reg.bias  torch.Size([5])\n",
            "roi_head.bbox_head.shared_fcs.0.weight  torch.Size([1024, 12544])\n",
            "roi_head.bbox_head.shared_fcs.0.bias  torch.Size([1024])\n",
            "roi_head.bbox_head.shared_fcs.1.weight  torch.Size([1024, 1024])\n",
            "roi_head.bbox_head.shared_fcs.1.bias  torch.Size([1024])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if __name__ == '__main__':\n",
        "    model_pth = r'D:\\Codes\\Python\\AI\\6_Model\\orcn-swin-t-dota-latest.pth'\n",
        "    net = torch.load(model_pth, map_location=torch.device('cpu'))\n",
        "    for key, value in net[\"state_dict\"].items():\n",
        "        print(key,value.size(),sep=\"  \")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PTH转为ONNX\n",
        "\n",
        "```python\n",
        "torch.onnx.export(\n",
        "    model,  # 要导出的模型\n",
        "    args, #  模型的输入参数,输入参数只需满足shape正确\n",
        "    onnx_export_filepath, # 转换输出的onnx模型的路径\n",
        "    export_params=True, # true表示导出trained model，否则untrained model。默认即可\n",
        "    verbose=False, #  true表示打印调试信息\n",
        "    input_names=None, # 指定输入节点名称\n",
        "    output_names=None, # 指定输出节点名称\n",
        "    do_constant_folding=True, # 是否使用常量折叠，默认即可\n",
        "    dynamic_axes=None, # 通过dynamic_axes来指定输入tensor的哪些参数可变\n",
        "    opset_version=9 # 指定onnx的opset版本，版本过低的话，不支持upsample等操作\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'eval'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCodes\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mPython\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mAI\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m6_Model\u001b[39m\u001b[39m\\\u001b[39m\u001b[39morcn-swin-t-dota-latest.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49meval()\n\u001b[0;32m      4\u001b[0m input_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m output_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.load(r'D:\\Codes\\Python\\AI\\6_Model\\orcn-swin-t-dota-latest.pth')\n",
        "model.eval()\n",
        "input_names = ['input']\n",
        "output_names = ['output']\n",
        "x = torch.randn(1,3,1150,1150,requires_grad=True)\n",
        "torch.onnx.export(model, x, r'D:\\Codes\\Python\\AI\\6_Model\\orcn-swin-t-dota-latest.onnx', input_names=input_names, output_names=output_names, verbose='True')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 利用PTH文件推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "def pth_push(img):\n",
        "    model=#这里调用你的模型\n",
        "    model_path=''#pth权重文件地址\n",
        "    device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')#cpu or gpu\n",
        "    model.load_state_dict(torch.load(self.model_path, map_location=device))#加载pth文件\n",
        "    model  = model.eval()\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "])#对图片进行resize并转换成tensor\n",
        "    inputs = transform(img)\n",
        "    inputs=torch.unsqueeze(inputs, 0)#添加一个维度\n",
        "    inputs = inputs.to(device)#把图片也转成相应的设备cuda or cpu\n",
        "    #进行推理\n",
        "    outputs = model(inputs)\n",
        "    #根据自己要解决的问题进行解码\n",
        "    outputs1=outputs.tolist()\n",
        "    outputs1 = torch.from_numpy(np.array(outputs1))\n",
        "    outputs_softmax = torch.softmax(outputs1, dim=1).numpy()[:, 1].tolist()[0]\n",
        "if __name__ == \"__main__\":  \n",
        "    i='.jpg' \n",
        "    image = Image.open(i)\n",
        "    pth_push(image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 模型推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"demo/video_demo.py\", line 96, in <module>\n",
            "    main()\n",
            "  File \"demo/video_demo.py\", line 52, in main\n",
            "    model = init_detector(args.config, args.checkpoint, device=args.device)\n",
            "  File \"d:\\codes\\python\\ai\\4_baseline\\openmmlab\\mmdetection\\mmdet\\apis\\inference.py\", line 53, in init_detector\n",
            "    config = Config.fromfile(config)\n",
            "  File \"c:\\Soft\\miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\config\\config.py\", line 460, in fromfile\n",
            "    lazy_import is None and not Config._is_lazy_import(filename):\n",
            "  File \"c:\\Soft\\miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\config\\config.py\", line 1661, in _is_lazy_import\n",
            "    with open(filename, encoding='utf-8') as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'configs/my/dino-4s_r50_rip.py'\n"
          ]
        }
      ],
      "source": [
        "! python demo/video_demo.py demo/houhai/video/rip_01.mp4 \\\n",
        "                                configs/my/dino-4s_r50_rip.py \\\n",
        "                                model/dino_4s_e12.pth.pth \\\n",
        "                                --out demo/vis/houhai-res/dino_4s_e12/0.05/video/rip_01.mp4 \\\n",
        "                                --score-thr 0.05 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: model/yolov5_0705.pth\n",
            "11/15 11:17:44 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/867, elapsed: 0s, ETA:\n",
            "[                                 ] 1/867, 0.2 task/s, elapsed: 4s, ETA:  3633s\n",
            "[                                 ] 2/867, 0.5 task/s, elapsed: 4s, ETA:  1865s\n",
            "[                                 ] 3/867, 0.7 task/s, elapsed: 4s, ETA:  1273s\n",
            "[                                 ] 4/867, 0.9 task/s, elapsed: 5s, ETA:   977s\n",
            "[                                 ] 5/867, 1.1 task/s, elapsed: 5s, ETA:   800s\n",
            "[                                 ] 6/867, 1.3 task/s, elapsed: 5s, ETA:   681s\n",
            "[                                 ] 7/867, 1.4 task/s, elapsed: 5s, ETA:   597s\n",
            "[                                 ] 8/867, 1.6 task/s, elapsed: 5s, ETA:   533s\n",
            "[                                 ] 9/867, 1.8 task/s, elapsed: 5s, ETA:   484s\n",
            "[                                ] 10/867, 1.9 task/s, elapsed: 5s, ETA:   444s\n",
            "[                                ] 11/867, 2.1 task/s, elapsed: 5s, ETA:   413s\n",
            "[                                ] 12/867, 2.2 task/s, elapsed: 5s, ETA:   387s\n",
            "[                                ] 13/867, 2.3 task/s, elapsed: 6s, ETA:   364s\n",
            "[                                ] 14/867, 2.5 task/s, elapsed: 6s, ETA:   345s\n",
            "[                                ] 15/867, 2.6 task/s, elapsed: 6s, ETA:   328s\n",
            "[                                ] 16/867, 2.7 task/s, elapsed: 6s, ETA:   313s\n",
            "[                                ] 17/867, 2.8 task/s, elapsed: 6s, ETA:   300s\n",
            "[                                ] 18/867, 2.9 task/s, elapsed: 6s, ETA:   288s\n",
            "[                                ] 19/867, 3.1 task/s, elapsed: 6s, ETA:   278s\n",
            "[                                ] 20/867, 3.2 task/s, elapsed: 6s, ETA:   268s\n",
            "[                                ] 21/867, 3.3 task/s, elapsed: 6s, ETA:   260s\n",
            "[                                ] 22/867, 3.4 task/s, elapsed: 7s, ETA:   252s\n",
            "[                                ] 23/867, 3.4 task/s, elapsed: 7s, ETA:   245s\n",
            "[                                ] 24/867, 3.5 task/s, elapsed: 7s, ETA:   238s\n",
            "[                                ] 25/867, 3.6 task/s, elapsed: 7s, ETA:   232s\n",
            "[                                ] 26/867, 3.7 task/s, elapsed: 7s, ETA:   226s\n",
            "[                                ] 27/867, 3.8 task/s, elapsed: 7s, ETA:   221s\n",
            "[>                               ] 28/867, 3.9 task/s, elapsed: 7s, ETA:   216s\n",
            "[>                               ] 29/867, 4.0 task/s, elapsed: 7s, ETA:   212s\n",
            "[>                               ] 30/867, 4.0 task/s, elapsed: 7s, ETA:   208s\n",
            "[>                               ] 31/867, 4.1 task/s, elapsed: 8s, ETA:   204s\n",
            "[>                               ] 32/867, 4.2 task/s, elapsed: 8s, ETA:   200s\n",
            "[>                               ] 33/867, 4.2 task/s, elapsed: 8s, ETA:   197s\n",
            "[>                               ] 34/867, 4.3 task/s, elapsed: 8s, ETA:   193s\n",
            "[>                               ] 35/867, 4.4 task/s, elapsed: 8s, ETA:   190s\n",
            "[>                               ] 36/867, 4.4 task/s, elapsed: 8s, ETA:   187s\n",
            "[>                               ] 37/867, 4.5 task/s, elapsed: 8s, ETA:   185s\n",
            "[>                               ] 38/867, 4.6 task/s, elapsed: 8s, ETA:   182s\n",
            "[>                               ] 39/867, 4.6 task/s, elapsed: 8s, ETA:   179s\n",
            "[>                               ] 40/867, 4.7 task/s, elapsed: 9s, ETA:   177s\n",
            "[>                               ] 41/867, 4.7 task/s, elapsed: 9s, ETA:   175s\n",
            "[>                               ] 42/867, 4.8 task/s, elapsed: 9s, ETA:   173s\n",
            "[>                               ] 43/867, 4.8 task/s, elapsed: 9s, ETA:   171s\n",
            "[>                               ] 44/867, 4.9 task/s, elapsed: 9s, ETA:   169s\n",
            "[>                               ] 45/867, 4.9 task/s, elapsed: 9s, ETA:   167s\n",
            "[>                               ] 46/867, 5.0 task/s, elapsed: 9s, ETA:   166s\n",
            "[>                               ] 47/867, 5.0 task/s, elapsed: 9s, ETA:   164s\n",
            "[>                              ] 48/867, 5.0 task/s, elapsed: 10s, ETA:   163s\n",
            "[>                              ] 49/867, 5.1 task/s, elapsed: 10s, ETA:   161s\n",
            "[>                              ] 50/867, 5.1 task/s, elapsed: 10s, ETA:   160s\n",
            "[>                              ] 51/867, 5.2 task/s, elapsed: 10s, ETA:   158s\n",
            "[>                              ] 52/867, 5.2 task/s, elapsed: 10s, ETA:   157s\n",
            "[>                              ] 53/867, 5.2 task/s, elapsed: 10s, ETA:   157s\n",
            "[>                              ] 54/867, 5.2 task/s, elapsed: 10s, ETA:   156s\n",
            "[>                              ] 55/867, 5.2 task/s, elapsed: 10s, ETA:   155s\n",
            "[>>                             ] 56/867, 5.3 task/s, elapsed: 11s, ETA:   154s\n",
            "[>>                             ] 57/867, 5.3 task/s, elapsed: 11s, ETA:   153s\n",
            "[>>                             ] 58/867, 5.3 task/s, elapsed: 11s, ETA:   152s\n",
            "[>>                             ] 59/867, 5.3 task/s, elapsed: 11s, ETA:   151s\n",
            "[>>                             ] 60/867, 5.4 task/s, elapsed: 11s, ETA:   150s\n",
            "[>>                             ] 61/867, 5.4 task/s, elapsed: 11s, ETA:   149s\n",
            "[>>                             ] 62/867, 5.5 task/s, elapsed: 11s, ETA:   148s\n",
            "[>>                             ] 63/867, 5.5 task/s, elapsed: 11s, ETA:   146s\n",
            "[>>                             ] 64/867, 5.5 task/s, elapsed: 12s, ETA:   145s\n",
            "[>>                             ] 65/867, 5.6 task/s, elapsed: 12s, ETA:   144s\n",
            "[>>                             ] 66/867, 5.6 task/s, elapsed: 12s, ETA:   143s\n",
            "[>>                             ] 67/867, 5.6 task/s, elapsed: 12s, ETA:   142s\n",
            "[>>                             ] 68/867, 5.7 task/s, elapsed: 12s, ETA:   141s\n",
            "[>>                             ] 69/867, 5.7 task/s, elapsed: 12s, ETA:   140s\n",
            "[>>                             ] 70/867, 5.7 task/s, elapsed: 12s, ETA:   139s\n",
            "[>>                             ] 71/867, 5.7 task/s, elapsed: 12s, ETA:   139s\n",
            "[>>                             ] 72/867, 5.8 task/s, elapsed: 12s, ETA:   138s\n",
            "[>>                             ] 73/867, 5.8 task/s, elapsed: 13s, ETA:   137s\n",
            "[>>                             ] 74/867, 5.8 task/s, elapsed: 13s, ETA:   136s\n",
            "[>>                             ] 75/867, 5.8 task/s, elapsed: 13s, ETA:   136s\n",
            "[>>                             ] 76/867, 5.9 task/s, elapsed: 13s, ETA:   135s\n",
            "[>>                             ] 77/867, 5.9 task/s, elapsed: 13s, ETA:   134s\n",
            "[>>                             ] 78/867, 5.9 task/s, elapsed: 13s, ETA:   134s\n",
            "[>>                             ] 79/867, 5.9 task/s, elapsed: 13s, ETA:   133s\n",
            "[>>                             ] 80/867, 6.0 task/s, elapsed: 13s, ETA:   132s\n",
            "[>>                             ] 81/867, 6.0 task/s, elapsed: 14s, ETA:   132s\n",
            "[>>                             ] 82/867, 6.0 task/s, elapsed: 14s, ETA:   131s\n",
            "[>>                             ] 83/867, 6.0 task/s, elapsed: 14s, ETA:   130s\n",
            "[>>>                            ] 84/867, 6.0 task/s, elapsed: 14s, ETA:   130s\n",
            "[>>>                            ] 85/867, 6.1 task/s, elapsed: 14s, ETA:   129s\n",
            "[>>>                            ] 86/867, 6.1 task/s, elapsed: 14s, ETA:   129s\n",
            "[>>>                            ] 87/867, 6.1 task/s, elapsed: 14s, ETA:   128s\n",
            "[>>>                            ] 88/867, 6.1 task/s, elapsed: 14s, ETA:   128s\n",
            "[>>>                            ] 89/867, 6.1 task/s, elapsed: 15s, ETA:   127s\n",
            "[>>>                            ] 90/867, 6.1 task/s, elapsed: 15s, ETA:   127s\n",
            "[>>>                            ] 91/867, 6.1 task/s, elapsed: 15s, ETA:   126s\n",
            "[>>>                            ] 92/867, 6.1 task/s, elapsed: 15s, ETA:   126s\n",
            "[>>>                            ] 93/867, 6.2 task/s, elapsed: 15s, ETA:   126s\n",
            "[>>>                            ] 94/867, 6.2 task/s, elapsed: 15s, ETA:   126s\n",
            "[>>>                            ] 95/867, 6.2 task/s, elapsed: 15s, ETA:   125s\n",
            "[>>>                            ] 96/867, 6.2 task/s, elapsed: 16s, ETA:   125s\n",
            "[>>>                            ] 97/867, 6.2 task/s, elapsed: 16s, ETA:   125s\n",
            "[>>>                            ] 98/867, 6.2 task/s, elapsed: 16s, ETA:   124s\n",
            "[>>>                            ] 99/867, 6.2 task/s, elapsed: 16s, ETA:   124s\n",
            "[>>>                           ] 100/867, 6.2 task/s, elapsed: 16s, ETA:   124s\n",
            "[>>>                           ] 101/867, 6.2 task/s, elapsed: 16s, ETA:   124s\n",
            "[>>>                           ] 102/867, 6.2 task/s, elapsed: 16s, ETA:   123s\n",
            "[>>>                           ] 103/867, 6.2 task/s, elapsed: 17s, ETA:   123s\n",
            "[>>>                           ] 104/867, 6.2 task/s, elapsed: 17s, ETA:   123s\n",
            "[>>>                           ] 105/867, 6.2 task/s, elapsed: 17s, ETA:   122s\n",
            "[>>>                           ] 106/867, 6.3 task/s, elapsed: 17s, ETA:   122s\n",
            "[>>>                           ] 107/867, 6.3 task/s, elapsed: 17s, ETA:   121s\n",
            "[>>>                           ] 108/867, 6.3 task/s, elapsed: 17s, ETA:   121s\n",
            "[>>>                           ] 109/867, 6.3 task/s, elapsed: 17s, ETA:   120s\n",
            "[>>>                           ] 110/867, 6.3 task/s, elapsed: 17s, ETA:   120s\n",
            "[>>>                           ] 111/867, 6.4 task/s, elapsed: 17s, ETA:   119s\n",
            "[>>>                           ] 112/867, 6.4 task/s, elapsed: 18s, ETA:   119s\n",
            "[>>>                           ] 113/867, 6.4 task/s, elapsed: 18s, ETA:   118s\n",
            "[>>>                           ] 114/867, 6.4 task/s, elapsed: 18s, ETA:   118s\n",
            "[>>>                           ] 115/867, 6.4 task/s, elapsed: 18s, ETA:   117s\n",
            "[>>>>                          ] 116/867, 6.4 task/s, elapsed: 18s, ETA:   117s\n",
            "[>>>>                          ] 117/867, 6.4 task/s, elapsed: 18s, ETA:   116s\n",
            "[>>>>                          ] 118/867, 6.5 task/s, elapsed: 18s, ETA:   116s\n",
            "[>>>>                          ] 119/867, 6.5 task/s, elapsed: 18s, ETA:   115s\n",
            "[>>>>                          ] 120/867, 6.5 task/s, elapsed: 18s, ETA:   115s\n",
            "[>>>>                          ] 121/867, 6.5 task/s, elapsed: 19s, ETA:   115s\n",
            "[>>>>                          ] 122/867, 6.5 task/s, elapsed: 19s, ETA:   114s\n",
            "[>>>>                          ] 123/867, 6.5 task/s, elapsed: 19s, ETA:   114s\n",
            "[>>>>                          ] 124/867, 6.5 task/s, elapsed: 19s, ETA:   114s\n",
            "[>>>>                          ] 125/867, 6.6 task/s, elapsed: 19s, ETA:   113s\n",
            "[>>>>                          ] 126/867, 6.6 task/s, elapsed: 19s, ETA:   113s\n",
            "[>>>>                          ] 127/867, 6.6 task/s, elapsed: 19s, ETA:   113s\n",
            "[>>>>                          ] 128/867, 6.6 task/s, elapsed: 19s, ETA:   112s\n",
            "[>>>>                          ] 129/867, 6.6 task/s, elapsed: 20s, ETA:   112s\n",
            "[>>>>                          ] 130/867, 6.6 task/s, elapsed: 20s, ETA:   112s\n",
            "[>>>>                          ] 131/867, 6.6 task/s, elapsed: 20s, ETA:   111s\n",
            "[>>>>                          ] 132/867, 6.6 task/s, elapsed: 20s, ETA:   111s\n",
            "[>>>>                          ] 133/867, 6.6 task/s, elapsed: 20s, ETA:   111s\n",
            "[>>>>                          ] 134/867, 6.6 task/s, elapsed: 20s, ETA:   110s\n",
            "[>>>>                          ] 135/867, 6.6 task/s, elapsed: 20s, ETA:   110s\n",
            "[>>>>                          ] 136/867, 6.7 task/s, elapsed: 20s, ETA:   110s\n",
            "[>>>>                          ] 137/867, 6.7 task/s, elapsed: 21s, ETA:   110s\n",
            "[>>>>                          ] 138/867, 6.7 task/s, elapsed: 21s, ETA:   109s\n",
            "[>>>>                          ] 139/867, 6.7 task/s, elapsed: 21s, ETA:   109s\n",
            "[>>>>                          ] 140/867, 6.7 task/s, elapsed: 21s, ETA:   109s\n",
            "[>>>>                          ] 141/867, 6.7 task/s, elapsed: 21s, ETA:   109s\n",
            "[>>>>                          ] 142/867, 6.7 task/s, elapsed: 21s, ETA:   109s\n",
            "[>>>>                          ] 143/867, 6.7 task/s, elapsed: 21s, ETA:   109s\n",
            "[>>>>                          ] 144/867, 6.7 task/s, elapsed: 22s, ETA:   108s\n",
            "[>>>>>                         ] 145/867, 6.7 task/s, elapsed: 22s, ETA:   108s\n",
            "[>>>>>                         ] 146/867, 6.7 task/s, elapsed: 22s, ETA:   108s\n",
            "[>>>>>                         ] 147/867, 6.7 task/s, elapsed: 22s, ETA:   108s\n",
            "[>>>>>                         ] 148/867, 6.7 task/s, elapsed: 22s, ETA:   108s\n",
            "[>>>>>                         ] 149/867, 6.7 task/s, elapsed: 22s, ETA:   107s\n",
            "[>>>>>                         ] 150/867, 6.7 task/s, elapsed: 22s, ETA:   107s\n",
            "[>>>>>                         ] 151/867, 6.7 task/s, elapsed: 22s, ETA:   107s\n",
            "[>>>>>                         ] 152/867, 6.7 task/s, elapsed: 23s, ETA:   106s\n",
            "[>>>>>                         ] 153/867, 6.7 task/s, elapsed: 23s, ETA:   106s\n",
            "[>>>>>                         ] 154/867, 6.8 task/s, elapsed: 23s, ETA:   106s\n",
            "[>>>>>                         ] 155/867, 6.8 task/s, elapsed: 23s, ETA:   105s\n",
            "[>>>>>                         ] 156/867, 6.8 task/s, elapsed: 23s, ETA:   105s\n",
            "[>>>>>                         ] 157/867, 6.8 task/s, elapsed: 23s, ETA:   105s\n",
            "[>>>>>                         ] 158/867, 6.8 task/s, elapsed: 23s, ETA:   104s\n",
            "[>>>>>                         ] 159/867, 6.8 task/s, elapsed: 23s, ETA:   104s\n",
            "[>>>>>                         ] 160/867, 6.8 task/s, elapsed: 23s, ETA:   104s\n",
            "[>>>>>                         ] 161/867, 6.8 task/s, elapsed: 24s, ETA:   103s\n",
            "[>>>>>                         ] 162/867, 6.8 task/s, elapsed: 24s, ETA:   103s\n",
            "[>>>>>                         ] 163/867, 6.9 task/s, elapsed: 24s, ETA:   103s\n",
            "[>>>>>                         ] 164/867, 6.9 task/s, elapsed: 24s, ETA:   102s\n",
            "[>>>>>                         ] 165/867, 6.9 task/s, elapsed: 24s, ETA:   102s\n",
            "[>>>>>                         ] 166/867, 6.9 task/s, elapsed: 24s, ETA:   102s\n",
            "[>>>>>                         ] 167/867, 6.9 task/s, elapsed: 24s, ETA:   102s\n",
            "[>>>>>                         ] 168/867, 6.9 task/s, elapsed: 24s, ETA:   101s\n",
            "[>>>>>                         ] 169/867, 6.9 task/s, elapsed: 24s, ETA:   101s\n",
            "[>>>>>                         ] 170/867, 6.9 task/s, elapsed: 25s, ETA:   101s\n",
            "[>>>>>                         ] 171/867, 6.9 task/s, elapsed: 25s, ETA:   101s\n",
            "[>>>>>                         ] 172/867, 6.9 task/s, elapsed: 25s, ETA:   100s\n",
            "[>>>>>                         ] 173/867, 6.9 task/s, elapsed: 25s, ETA:   100s\n",
            "[>>>>>>                        ] 174/867, 6.9 task/s, elapsed: 25s, ETA:   100s\n",
            "[>>>>>>                        ] 175/867, 6.9 task/s, elapsed: 25s, ETA:   100s\n",
            "[>>>>>>                        ] 176/867, 6.9 task/s, elapsed: 25s, ETA:   100s\n",
            "[>>>>>>                        ] 177/867, 6.9 task/s, elapsed: 26s, ETA:   100s\n",
            "[>>>>>>                        ] 178/867, 6.9 task/s, elapsed: 26s, ETA:   100s\n",
            "[>>>>>>                        ] 179/867, 6.9 task/s, elapsed: 26s, ETA:    99s\n",
            "[>>>>>>                        ] 180/867, 6.9 task/s, elapsed: 26s, ETA:    99s\n",
            "[>>>>>>                        ] 181/867, 6.9 task/s, elapsed: 26s, ETA:    99s\n",
            "[>>>>>>                        ] 182/867, 6.9 task/s, elapsed: 26s, ETA:    99s\n",
            "[>>>>>>                        ] 183/867, 7.0 task/s, elapsed: 26s, ETA:    98s\n",
            "[>>>>>>                        ] 184/867, 7.0 task/s, elapsed: 26s, ETA:    98s\n",
            "[>>>>>>                        ] 185/867, 7.0 task/s, elapsed: 27s, ETA:    98s\n",
            "[>>>>>>                        ] 186/867, 7.0 task/s, elapsed: 27s, ETA:    98s\n",
            "[>>>>>>                        ] 187/867, 7.0 task/s, elapsed: 27s, ETA:    97s\n",
            "[>>>>>>                        ] 188/867, 7.0 task/s, elapsed: 27s, ETA:    97s\n",
            "[>>>>>>                        ] 189/867, 7.0 task/s, elapsed: 27s, ETA:    97s\n",
            "[>>>>>>                        ] 190/867, 7.0 task/s, elapsed: 27s, ETA:    97s\n",
            "[>>>>>>                        ] 191/867, 7.0 task/s, elapsed: 27s, ETA:    96s\n",
            "[>>>>>>                        ] 192/867, 7.0 task/s, elapsed: 27s, ETA:    96s\n",
            "[>>>>>>                        ] 193/867, 7.0 task/s, elapsed: 27s, ETA:    96s\n",
            "[>>>>>>                        ] 194/867, 7.0 task/s, elapsed: 28s, ETA:    96s\n",
            "[>>>>>>                        ] 195/867, 7.0 task/s, elapsed: 28s, ETA:    95s\n",
            "[>>>>>>                        ] 196/867, 7.1 task/s, elapsed: 28s, ETA:    95s\n",
            "[>>>>>>                        ] 197/867, 7.1 task/s, elapsed: 28s, ETA:    95s\n",
            "[>>>>>>                        ] 198/867, 7.1 task/s, elapsed: 28s, ETA:    95s\n",
            "[>>>>>>                        ] 199/867, 7.1 task/s, elapsed: 28s, ETA:    95s\n",
            "[>>>>>>                        ] 200/867, 7.1 task/s, elapsed: 28s, ETA:    94s\n",
            "[>>>>>>                        ] 201/867, 7.1 task/s, elapsed: 28s, ETA:    94s\n",
            "[>>>>>>                        ] 202/867, 7.1 task/s, elapsed: 29s, ETA:    94s\n",
            "[>>>>>>>                       ] 203/867, 7.1 task/s, elapsed: 29s, ETA:    94s\n",
            "[>>>>>>>                       ] 204/867, 7.1 task/s, elapsed: 29s, ETA:    94s\n",
            "[>>>>>>>                       ] 205/867, 7.1 task/s, elapsed: 29s, ETA:    93s\n",
            "[>>>>>>>                       ] 206/867, 7.1 task/s, elapsed: 29s, ETA:    93s\n",
            "[>>>>>>>                       ] 207/867, 7.1 task/s, elapsed: 29s, ETA:    93s\n",
            "[>>>>>>>                       ] 208/867, 7.1 task/s, elapsed: 29s, ETA:    93s\n",
            "[>>>>>>>                       ] 209/867, 7.1 task/s, elapsed: 29s, ETA:    93s\n",
            "[>>>>>>>                       ] 210/867, 7.1 task/s, elapsed: 30s, ETA:    92s\n",
            "[>>>>>>>                       ] 211/867, 7.1 task/s, elapsed: 30s, ETA:    92s\n",
            "[>>>>>>>                       ] 212/867, 7.1 task/s, elapsed: 30s, ETA:    92s\n",
            "[>>>>>>>                       ] 213/867, 7.1 task/s, elapsed: 30s, ETA:    92s\n",
            "[>>>>>>>                       ] 214/867, 7.1 task/s, elapsed: 30s, ETA:    92s\n",
            "[>>>>>>>                       ] 215/867, 7.1 task/s, elapsed: 30s, ETA:    92s\n",
            "[>>>>>>>                       ] 216/867, 7.1 task/s, elapsed: 30s, ETA:    91s\n",
            "[>>>>>>>                       ] 217/867, 7.1 task/s, elapsed: 30s, ETA:    91s\n",
            "[>>>>>>>                       ] 218/867, 7.1 task/s, elapsed: 31s, ETA:    91s\n",
            "[>>>>>>>                       ] 219/867, 7.1 task/s, elapsed: 31s, ETA:    91s\n",
            "[>>>>>>>                       ] 220/867, 7.1 task/s, elapsed: 31s, ETA:    91s\n",
            "[>>>>>>>                       ] 221/867, 7.1 task/s, elapsed: 31s, ETA:    91s\n",
            "[>>>>>>>                       ] 222/867, 7.1 task/s, elapsed: 31s, ETA:    91s\n",
            "[>>>>>>>                       ] 223/867, 7.1 task/s, elapsed: 31s, ETA:    90s\n",
            "[>>>>>>>                       ] 224/867, 7.1 task/s, elapsed: 31s, ETA:    90s\n",
            "[>>>>>>>                       ] 225/867, 7.1 task/s, elapsed: 32s, ETA:    90s\n",
            "[>>>>>>>                       ] 226/867, 7.1 task/s, elapsed: 32s, ETA:    90s\n",
            "[>>>>>>>                       ] 227/867, 7.1 task/s, elapsed: 32s, ETA:    90s\n",
            "[>>>>>>>                       ] 228/867, 7.1 task/s, elapsed: 32s, ETA:    89s\n",
            "[>>>>>>>                       ] 229/867, 7.2 task/s, elapsed: 32s, ETA:    89s\n",
            "[>>>>>>>                       ] 230/867, 7.2 task/s, elapsed: 32s, ETA:    89s\n",
            "[>>>>>>>                       ] 231/867, 7.2 task/s, elapsed: 32s, ETA:    89s\n",
            "[>>>>>>>>                      ] 232/867, 7.2 task/s, elapsed: 32s, ETA:    89s\n",
            "[>>>>>>>>                      ] 233/867, 7.2 task/s, elapsed: 32s, ETA:    88s\n",
            "[>>>>>>>>                      ] 234/867, 7.2 task/s, elapsed: 33s, ETA:    88s\n",
            "[>>>>>>>>                      ] 235/867, 7.2 task/s, elapsed: 33s, ETA:    88s\n",
            "[>>>>>>>>                      ] 236/867, 7.2 task/s, elapsed: 33s, ETA:    88s\n",
            "[>>>>>>>>                      ] 237/867, 7.2 task/s, elapsed: 33s, ETA:    87s\n",
            "[>>>>>>>>                      ] 238/867, 7.2 task/s, elapsed: 33s, ETA:    87s\n",
            "[>>>>>>>>                      ] 239/867, 7.2 task/s, elapsed: 33s, ETA:    87s\n",
            "[>>>>>>>>                      ] 240/867, 7.2 task/s, elapsed: 33s, ETA:    87s\n",
            "[>>>>>>>>                      ] 241/867, 7.2 task/s, elapsed: 33s, ETA:    87s\n",
            "[>>>>>>>>                      ] 242/867, 7.2 task/s, elapsed: 33s, ETA:    86s\n",
            "[>>>>>>>>                      ] 243/867, 7.2 task/s, elapsed: 34s, ETA:    86s\n",
            "[>>>>>>>>                      ] 244/867, 7.2 task/s, elapsed: 34s, ETA:    86s\n",
            "[>>>>>>>>                      ] 245/867, 7.2 task/s, elapsed: 34s, ETA:    86s\n",
            "[>>>>>>>>                      ] 246/867, 7.2 task/s, elapsed: 34s, ETA:    86s\n",
            "[>>>>>>>>                      ] 247/867, 7.2 task/s, elapsed: 34s, ETA:    86s\n",
            "[>>>>>>>>                      ] 248/867, 7.2 task/s, elapsed: 34s, ETA:    86s\n",
            "[>>>>>>>>                      ] 249/867, 7.2 task/s, elapsed: 34s, ETA:    85s\n",
            "[>>>>>>>>                      ] 250/867, 7.2 task/s, elapsed: 35s, ETA:    85s\n",
            "[>>>>>>>>                      ] 251/867, 7.2 task/s, elapsed: 35s, ETA:    85s\n",
            "[>>>>>>>>                      ] 252/867, 7.3 task/s, elapsed: 35s, ETA:    85s\n",
            "[>>>>>>>>                      ] 253/867, 7.3 task/s, elapsed: 35s, ETA:    85s\n",
            "[>>>>>>>>                      ] 254/867, 7.3 task/s, elapsed: 35s, ETA:    84s\n",
            "[>>>>>>>>                      ] 255/867, 7.3 task/s, elapsed: 35s, ETA:    84s\n",
            "[>>>>>>>>                      ] 256/867, 7.3 task/s, elapsed: 35s, ETA:    84s\n",
            "[>>>>>>>>                      ] 257/867, 7.3 task/s, elapsed: 35s, ETA:    84s\n",
            "[>>>>>>>>                      ] 258/867, 7.3 task/s, elapsed: 35s, ETA:    84s\n",
            "[>>>>>>>>                      ] 259/867, 7.3 task/s, elapsed: 36s, ETA:    83s\n",
            "[>>>>>>>>                      ] 260/867, 7.3 task/s, elapsed: 36s, ETA:    83s\n",
            "[>>>>>>>>>                     ] 261/867, 7.3 task/s, elapsed: 36s, ETA:    83s\n",
            "[>>>>>>>>>                     ] 262/867, 7.3 task/s, elapsed: 36s, ETA:    83s\n",
            "[>>>>>>>>>                     ] 263/867, 7.3 task/s, elapsed: 36s, ETA:    83s\n",
            "[>>>>>>>>>                     ] 264/867, 7.3 task/s, elapsed: 36s, ETA:    83s\n",
            "[>>>>>>>>>                     ] 265/867, 7.3 task/s, elapsed: 36s, ETA:    82s\n",
            "[>>>>>>>>>                     ] 266/867, 7.3 task/s, elapsed: 36s, ETA:    82s\n",
            "[>>>>>>>>>                     ] 267/867, 7.3 task/s, elapsed: 37s, ETA:    82s\n",
            "[>>>>>>>>>                     ] 268/867, 7.3 task/s, elapsed: 37s, ETA:    82s\n",
            "[>>>>>>>>>                     ] 269/867, 7.3 task/s, elapsed: 37s, ETA:    82s\n",
            "[>>>>>>>>>                     ] 270/867, 7.3 task/s, elapsed: 37s, ETA:    82s\n",
            "[>>>>>>>>>                     ] 271/867, 7.3 task/s, elapsed: 37s, ETA:    82s\n",
            "[>>>>>>>>>                     ] 272/867, 7.3 task/s, elapsed: 37s, ETA:    81s\n",
            "[>>>>>>>>>                     ] 273/867, 7.3 task/s, elapsed: 37s, ETA:    81s\n",
            "[>>>>>>>>>                     ] 274/867, 7.3 task/s, elapsed: 37s, ETA:    81s\n",
            "[>>>>>>>>>                     ] 275/867, 7.3 task/s, elapsed: 38s, ETA:    81s\n",
            "[>>>>>>>>>                     ] 276/867, 7.3 task/s, elapsed: 38s, ETA:    81s\n",
            "[>>>>>>>>>                     ] 277/867, 7.3 task/s, elapsed: 38s, ETA:    80s\n",
            "[>>>>>>>>>                     ] 278/867, 7.3 task/s, elapsed: 38s, ETA:    80s\n",
            "[>>>>>>>>>                     ] 279/867, 7.3 task/s, elapsed: 38s, ETA:    80s\n",
            "[>>>>>>>>>                     ] 280/867, 7.4 task/s, elapsed: 38s, ETA:    80s\n",
            "[>>>>>>>>>                     ] 281/867, 7.4 task/s, elapsed: 38s, ETA:    80s\n",
            "[>>>>>>>>>                     ] 282/867, 7.4 task/s, elapsed: 38s, ETA:    79s\n",
            "[>>>>>>>>>                     ] 283/867, 7.4 task/s, elapsed: 38s, ETA:    79s\n",
            "[>>>>>>>>>                     ] 284/867, 7.4 task/s, elapsed: 39s, ETA:    79s\n",
            "[>>>>>>>>>                     ] 285/867, 7.4 task/s, elapsed: 39s, ETA:    79s\n",
            "[>>>>>>>>>                     ] 286/867, 7.4 task/s, elapsed: 39s, ETA:    79s\n",
            "[>>>>>>>>>                     ] 287/867, 7.4 task/s, elapsed: 39s, ETA:    79s\n",
            "[>>>>>>>>>                     ] 288/867, 7.4 task/s, elapsed: 39s, ETA:    78s\n",
            "[>>>>>>>>>>                    ] 289/867, 7.4 task/s, elapsed: 39s, ETA:    78s\n",
            "[>>>>>>>>>>                    ] 290/867, 7.4 task/s, elapsed: 39s, ETA:    78s\n",
            "[>>>>>>>>>>                    ] 291/867, 7.4 task/s, elapsed: 39s, ETA:    78s\n",
            "[>>>>>>>>>>                    ] 292/867, 7.4 task/s, elapsed: 40s, ETA:    78s\n",
            "[>>>>>>>>>>                    ] 293/867, 7.4 task/s, elapsed: 40s, ETA:    78s\n",
            "[>>>>>>>>>>                    ] 294/867, 7.4 task/s, elapsed: 40s, ETA:    77s\n",
            "[>>>>>>>>>>                    ] 295/867, 7.4 task/s, elapsed: 40s, ETA:    77s\n",
            "[>>>>>>>>>>                    ] 296/867, 7.4 task/s, elapsed: 40s, ETA:    77s\n",
            "[>>>>>>>>>>                    ] 297/867, 7.4 task/s, elapsed: 40s, ETA:    77s\n",
            "[>>>>>>>>>>                    ] 298/867, 7.4 task/s, elapsed: 40s, ETA:    77s\n",
            "[>>>>>>>>>>                    ] 299/867, 7.4 task/s, elapsed: 40s, ETA:    77s\n",
            "[>>>>>>>>>>                    ] 300/867, 7.4 task/s, elapsed: 41s, ETA:    77s\n",
            "[>>>>>>>>>>                    ] 301/867, 7.4 task/s, elapsed: 41s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 302/867, 7.4 task/s, elapsed: 41s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 303/867, 7.4 task/s, elapsed: 41s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 304/867, 7.4 task/s, elapsed: 41s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 305/867, 7.4 task/s, elapsed: 41s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 306/867, 7.4 task/s, elapsed: 41s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 307/867, 7.4 task/s, elapsed: 42s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 308/867, 7.4 task/s, elapsed: 42s, ETA:    76s\n",
            "[>>>>>>>>>>                    ] 309/867, 7.4 task/s, elapsed: 42s, ETA:    75s\n",
            "[>>>>>>>>>>                    ] 310/867, 7.4 task/s, elapsed: 42s, ETA:    75s\n",
            "[>>>>>>>>>>                    ] 311/867, 7.4 task/s, elapsed: 42s, ETA:    75s\n",
            "[>>>>>>>>>>                    ] 312/867, 7.4 task/s, elapsed: 42s, ETA:    75s\n",
            "[>>>>>>>>>>                    ] 313/867, 7.4 task/s, elapsed: 42s, ETA:    75s\n",
            "[>>>>>>>>>>                    ] 314/867, 7.4 task/s, elapsed: 42s, ETA:    75s\n",
            "[>>>>>>>>>>                    ] 315/867, 7.4 task/s, elapsed: 42s, ETA:    74s\n",
            "[>>>>>>>>>>                    ] 316/867, 7.4 task/s, elapsed: 43s, ETA:    74s\n",
            "[>>>>>>>>>>                    ] 317/867, 7.4 task/s, elapsed: 43s, ETA:    74s\n",
            "[>>>>>>>>>>>                   ] 318/867, 7.4 task/s, elapsed: 43s, ETA:    74s\n",
            "[>>>>>>>>>>>                   ] 319/867, 7.4 task/s, elapsed: 43s, ETA:    74s\n",
            "[>>>>>>>>>>>                   ] 320/867, 7.4 task/s, elapsed: 43s, ETA:    74s\n",
            "[>>>>>>>>>>>                   ] 321/867, 7.4 task/s, elapsed: 43s, ETA:    73s\n",
            "[>>>>>>>>>>>                   ] 322/867, 7.4 task/s, elapsed: 43s, ETA:    73s\n",
            "[>>>>>>>>>>>                   ] 323/867, 7.4 task/s, elapsed: 43s, ETA:    73s\n",
            "[>>>>>>>>>>>                   ] 324/867, 7.5 task/s, elapsed: 43s, ETA:    73s\n",
            "[>>>>>>>>>>>                   ] 325/867, 7.5 task/s, elapsed: 44s, ETA:    73s\n",
            "[>>>>>>>>>>>                   ] 326/867, 7.5 task/s, elapsed: 44s, ETA:    73s\n",
            "[>>>>>>>>>>>                   ] 327/867, 7.5 task/s, elapsed: 44s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 328/867, 7.5 task/s, elapsed: 44s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 329/867, 7.5 task/s, elapsed: 44s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 330/867, 7.5 task/s, elapsed: 44s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 331/867, 7.5 task/s, elapsed: 44s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 332/867, 7.5 task/s, elapsed: 45s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 333/867, 7.5 task/s, elapsed: 45s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 334/867, 7.5 task/s, elapsed: 45s, ETA:    72s\n",
            "[>>>>>>>>>>>                   ] 335/867, 7.5 task/s, elapsed: 45s, ETA:    71s\n",
            "[>>>>>>>>>>>                   ] 336/867, 7.5 task/s, elapsed: 45s, ETA:    71s\n",
            "[>>>>>>>>>>>                   ] 337/867, 7.5 task/s, elapsed: 45s, ETA:    71s\n",
            "[>>>>>>>>>>>                   ] 338/867, 7.5 task/s, elapsed: 45s, ETA:    71s\n",
            "[>>>>>>>>>>>                   ] 339/867, 7.5 task/s, elapsed: 45s, ETA:    71s\n",
            "[>>>>>>>>>>>                   ] 340/867, 7.5 task/s, elapsed: 46s, ETA:    71s\n",
            "[>>>>>>>>>>>                   ] 341/867, 7.5 task/s, elapsed: 46s, ETA:    70s\n",
            "[>>>>>>>>>>>                   ] 342/867, 7.5 task/s, elapsed: 46s, ETA:    70s\n",
            "[>>>>>>>>>>>                   ] 343/867, 7.5 task/s, elapsed: 46s, ETA:    70s\n",
            "[>>>>>>>>>>>                   ] 344/867, 7.5 task/s, elapsed: 46s, ETA:    70s\n",
            "[>>>>>>>>>>>                   ] 345/867, 7.5 task/s, elapsed: 46s, ETA:    70s\n",
            "[>>>>>>>>>>>                   ] 346/867, 7.5 task/s, elapsed: 46s, ETA:    70s\n",
            "[>>>>>>>>>>>>                  ] 347/867, 7.5 task/s, elapsed: 46s, ETA:    69s\n",
            "[>>>>>>>>>>>>                  ] 348/867, 7.5 task/s, elapsed: 46s, ETA:    69s\n",
            "[>>>>>>>>>>>>                  ] 349/867, 7.5 task/s, elapsed: 47s, ETA:    69s\n",
            "[>>>>>>>>>>>>                  ] 350/867, 7.5 task/s, elapsed: 47s, ETA:    69s\n",
            "[>>>>>>>>>>>>                  ] 351/867, 7.5 task/s, elapsed: 47s, ETA:    69s\n",
            "[>>>>>>>>>>>>                  ] 352/867, 7.5 task/s, elapsed: 47s, ETA:    69s\n",
            "[>>>>>>>>>>>>                  ] 353/867, 7.5 task/s, elapsed: 47s, ETA:    68s\n",
            "[>>>>>>>>>>>>                  ] 354/867, 7.5 task/s, elapsed: 47s, ETA:    68s\n",
            "[>>>>>>>>>>>>                  ] 355/867, 7.5 task/s, elapsed: 47s, ETA:    68s\n",
            "[>>>>>>>>>>>>                  ] 356/867, 7.5 task/s, elapsed: 47s, ETA:    68s\n",
            "[>>>>>>>>>>>>                  ] 357/867, 7.5 task/s, elapsed: 48s, ETA:    68s\n",
            "[>>>>>>>>>>>>                  ] 358/867, 7.5 task/s, elapsed: 48s, ETA:    68s\n",
            "[>>>>>>>>>>>>                  ] 359/867, 7.5 task/s, elapsed: 48s, ETA:    68s\n",
            "[>>>>>>>>>>>>                  ] 360/867, 7.5 task/s, elapsed: 48s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 361/867, 7.5 task/s, elapsed: 48s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 362/867, 7.5 task/s, elapsed: 48s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 363/867, 7.5 task/s, elapsed: 48s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 364/867, 7.5 task/s, elapsed: 48s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 365/867, 7.5 task/s, elapsed: 49s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 366/867, 7.5 task/s, elapsed: 49s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 367/867, 7.5 task/s, elapsed: 49s, ETA:    67s\n",
            "[>>>>>>>>>>>>                  ] 368/867, 7.5 task/s, elapsed: 49s, ETA:    66s\n",
            "[>>>>>>>>>>>>                  ] 369/867, 7.5 task/s, elapsed: 49s, ETA:    66s\n",
            "[>>>>>>>>>>>>                  ] 370/867, 7.5 task/s, elapsed: 49s, ETA:    66s\n",
            "[>>>>>>>>>>>>                  ] 371/867, 7.5 task/s, elapsed: 49s, ETA:    66s\n",
            "[>>>>>>>>>>>>                  ] 372/867, 7.5 task/s, elapsed: 49s, ETA:    66s\n",
            "[>>>>>>>>>>>>                  ] 373/867, 7.5 task/s, elapsed: 50s, ETA:    66s\n",
            "[>>>>>>>>>>>>                  ] 374/867, 7.5 task/s, elapsed: 50s, ETA:    65s\n",
            "[>>>>>>>>>>>>                  ] 375/867, 7.5 task/s, elapsed: 50s, ETA:    65s\n",
            "[>>>>>>>>>>>>>                 ] 376/867, 7.5 task/s, elapsed: 50s, ETA:    65s\n",
            "[>>>>>>>>>>>>>                 ] 377/867, 7.5 task/s, elapsed: 50s, ETA:    65s\n",
            "[>>>>>>>>>>>>>                 ] 378/867, 7.5 task/s, elapsed: 50s, ETA:    65s\n",
            "[>>>>>>>>>>>>>                 ] 379/867, 7.5 task/s, elapsed: 50s, ETA:    65s\n",
            "[>>>>>>>>>>>>>                 ] 380/867, 7.6 task/s, elapsed: 50s, ETA:    64s\n",
            "[>>>>>>>>>>>>>                 ] 381/867, 7.6 task/s, elapsed: 50s, ETA:    64s\n",
            "[>>>>>>>>>>>>>                 ] 382/867, 7.6 task/s, elapsed: 51s, ETA:    64s\n",
            "[>>>>>>>>>>>>>                 ] 383/867, 7.6 task/s, elapsed: 51s, ETA:    64s\n",
            "[>>>>>>>>>>>>>                 ] 384/867, 7.6 task/s, elapsed: 51s, ETA:    64s\n",
            "[>>>>>>>>>>>>>                 ] 385/867, 7.6 task/s, elapsed: 51s, ETA:    64s\n",
            "[>>>>>>>>>>>>>                 ] 386/867, 7.6 task/s, elapsed: 51s, ETA:    64s\n",
            "[>>>>>>>>>>>>>                 ] 387/867, 7.6 task/s, elapsed: 51s, ETA:    63s\n",
            "[>>>>>>>>>>>>>                 ] 388/867, 7.6 task/s, elapsed: 51s, ETA:    63s\n",
            "[>>>>>>>>>>>>>                 ] 389/867, 7.6 task/s, elapsed: 51s, ETA:    63s\n",
            "[>>>>>>>>>>>>>                 ] 390/867, 7.6 task/s, elapsed: 51s, ETA:    63s\n",
            "[>>>>>>>>>>>>>                 ] 391/867, 7.6 task/s, elapsed: 52s, ETA:    63s\n",
            "[>>>>>>>>>>>>>                 ] 392/867, 7.6 task/s, elapsed: 52s, ETA:    63s\n",
            "[>>>>>>>>>>>>>                 ] 393/867, 7.6 task/s, elapsed: 52s, ETA:    63s\n",
            "[>>>>>>>>>>>>>                 ] 394/867, 7.6 task/s, elapsed: 52s, ETA:    62s\n",
            "[>>>>>>>>>>>>>                 ] 395/867, 7.6 task/s, elapsed: 52s, ETA:    62s\n",
            "[>>>>>>>>>>>>>                 ] 396/867, 7.6 task/s, elapsed: 52s, ETA:    62s\n",
            "[>>>>>>>>>>>>>                 ] 397/867, 7.6 task/s, elapsed: 52s, ETA:    62s\n",
            "[>>>>>>>>>>>>>                 ] 398/867, 7.6 task/s, elapsed: 52s, ETA:    62s\n",
            "[>>>>>>>>>>>>>                 ] 399/867, 7.6 task/s, elapsed: 53s, ETA:    62s\n",
            "[>>>>>>>>>>>>>                 ] 400/867, 7.6 task/s, elapsed: 53s, ETA:    62s\n",
            "[>>>>>>>>>>>>>                 ] 401/867, 7.6 task/s, elapsed: 53s, ETA:    61s\n",
            "[>>>>>>>>>>>>>                 ] 402/867, 7.6 task/s, elapsed: 53s, ETA:    61s\n",
            "[>>>>>>>>>>>>>                 ] 403/867, 7.6 task/s, elapsed: 53s, ETA:    61s\n",
            "[>>>>>>>>>>>>>                 ] 404/867, 7.6 task/s, elapsed: 53s, ETA:    61s\n",
            "[>>>>>>>>>>>>>>                ] 405/867, 7.6 task/s, elapsed: 53s, ETA:    61s\n",
            "[>>>>>>>>>>>>>>                ] 406/867, 7.6 task/s, elapsed: 54s, ETA:    61s\n",
            "[>>>>>>>>>>>>>>                ] 407/867, 7.6 task/s, elapsed: 54s, ETA:    61s\n",
            "[>>>>>>>>>>>>>>                ] 408/867, 7.6 task/s, elapsed: 54s, ETA:    61s\n",
            "[>>>>>>>>>>>>>>                ] 409/867, 7.6 task/s, elapsed: 54s, ETA:    60s\n",
            "[>>>>>>>>>>>>>>                ] 410/867, 7.6 task/s, elapsed: 54s, ETA:    60s\n",
            "[>>>>>>>>>>>>>>                ] 411/867, 7.6 task/s, elapsed: 54s, ETA:    60s\n",
            "[>>>>>>>>>>>>>>                ] 412/867, 7.6 task/s, elapsed: 54s, ETA:    60s\n",
            "[>>>>>>>>>>>>>>                ] 413/867, 7.6 task/s, elapsed: 54s, ETA:    60s\n",
            "[>>>>>>>>>>>>>>                ] 414/867, 7.6 task/s, elapsed: 55s, ETA:    60s\n",
            "[>>>>>>>>>>>>>>                ] 415/867, 7.6 task/s, elapsed: 55s, ETA:    60s\n",
            "[>>>>>>>>>>>>>>                ] 416/867, 7.6 task/s, elapsed: 55s, ETA:    59s\n",
            "[>>>>>>>>>>>>>>                ] 417/867, 7.6 task/s, elapsed: 55s, ETA:    59s\n",
            "[>>>>>>>>>>>>>>                ] 418/867, 7.6 task/s, elapsed: 55s, ETA:    59s\n",
            "[>>>>>>>>>>>>>>                ] 419/867, 7.6 task/s, elapsed: 55s, ETA:    59s\n",
            "[>>>>>>>>>>>>>>                ] 420/867, 7.6 task/s, elapsed: 55s, ETA:    59s\n",
            "[>>>>>>>>>>>>>>                ] 421/867, 7.6 task/s, elapsed: 55s, ETA:    59s\n",
            "[>>>>>>>>>>>>>>                ] 422/867, 7.6 task/s, elapsed: 56s, ETA:    59s\n",
            "[>>>>>>>>>>>>>>                ] 423/867, 7.6 task/s, elapsed: 56s, ETA:    58s\n",
            "[>>>>>>>>>>>>>>                ] 424/867, 7.6 task/s, elapsed: 56s, ETA:    58s\n",
            "[>>>>>>>>>>>>>>                ] 425/867, 7.6 task/s, elapsed: 56s, ETA:    58s\n",
            "[>>>>>>>>>>>>>>                ] 426/867, 7.6 task/s, elapsed: 56s, ETA:    58s\n",
            "[>>>>>>>>>>>>>>                ] 427/867, 7.6 task/s, elapsed: 56s, ETA:    58s\n",
            "[>>>>>>>>>>>>>>                ] 428/867, 7.6 task/s, elapsed: 56s, ETA:    58s\n",
            "[>>>>>>>>>>>>>>                ] 429/867, 7.6 task/s, elapsed: 56s, ETA:    58s\n",
            "[>>>>>>>>>>>>>>                ] 430/867, 7.6 task/s, elapsed: 56s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>                ] 431/867, 7.6 task/s, elapsed: 57s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>                ] 432/867, 7.6 task/s, elapsed: 57s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>                ] 433/867, 7.6 task/s, elapsed: 57s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>>               ] 434/867, 7.6 task/s, elapsed: 57s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>>               ] 435/867, 7.6 task/s, elapsed: 57s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>>               ] 436/867, 7.6 task/s, elapsed: 57s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>>               ] 437/867, 7.6 task/s, elapsed: 57s, ETA:    57s\n",
            "[>>>>>>>>>>>>>>>               ] 438/867, 7.6 task/s, elapsed: 58s, ETA:    56s\n",
            "[>>>>>>>>>>>>>>>               ] 439/867, 7.6 task/s, elapsed: 58s, ETA:    56s\n",
            "[>>>>>>>>>>>>>>>               ] 440/867, 7.6 task/s, elapsed: 58s, ETA:    56s\n",
            "[>>>>>>>>>>>>>>>               ] 441/867, 7.6 task/s, elapsed: 58s, ETA:    56s\n",
            "[>>>>>>>>>>>>>>>               ] 442/867, 7.6 task/s, elapsed: 58s, ETA:    56s\n",
            "[>>>>>>>>>>>>>>>               ] 443/867, 7.6 task/s, elapsed: 58s, ETA:    56s\n",
            "[>>>>>>>>>>>>>>>               ] 444/867, 7.6 task/s, elapsed: 58s, ETA:    56s\n",
            "[>>>>>>>>>>>>>>>               ] 445/867, 7.6 task/s, elapsed: 58s, ETA:    55s\n",
            "[>>>>>>>>>>>>>>>               ] 446/867, 7.6 task/s, elapsed: 58s, ETA:    55s\n",
            "[>>>>>>>>>>>>>>>               ] 447/867, 7.6 task/s, elapsed: 59s, ETA:    55s\n",
            "[>>>>>>>>>>>>>>>               ] 448/867, 7.6 task/s, elapsed: 59s, ETA:    55s\n",
            "[>>>>>>>>>>>>>>>               ] 449/867, 7.6 task/s, elapsed: 59s, ETA:    55s\n",
            "[>>>>>>>>>>>>>>>               ] 450/867, 7.6 task/s, elapsed: 59s, ETA:    55s\n",
            "[>>>>>>>>>>>>>>>               ] 451/867, 7.6 task/s, elapsed: 59s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 452/867, 7.6 task/s, elapsed: 59s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 453/867, 7.6 task/s, elapsed: 59s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 454/867, 7.6 task/s, elapsed: 59s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 455/867, 7.6 task/s, elapsed: 60s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 456/867, 7.6 task/s, elapsed: 60s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 457/867, 7.6 task/s, elapsed: 60s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 458/867, 7.6 task/s, elapsed: 60s, ETA:    54s\n",
            "[>>>>>>>>>>>>>>>               ] 459/867, 7.6 task/s, elapsed: 60s, ETA:    53s\n",
            "[>>>>>>>>>>>>>>>               ] 460/867, 7.6 task/s, elapsed: 60s, ETA:    53s\n",
            "[>>>>>>>>>>>>>>>               ] 461/867, 7.6 task/s, elapsed: 60s, ETA:    53s\n",
            "[>>>>>>>>>>>>>>>               ] 462/867, 7.6 task/s, elapsed: 60s, ETA:    53s\n",
            "[>>>>>>>>>>>>>>>>              ] 463/867, 7.6 task/s, elapsed: 61s, ETA:    53s\n",
            "[>>>>>>>>>>>>>>>>              ] 464/867, 7.6 task/s, elapsed: 61s, ETA:    53s\n",
            "[>>>>>>>>>>>>>>>>              ] 465/867, 7.6 task/s, elapsed: 61s, ETA:    53s\n",
            "[>>>>>>>>>>>>>>>>              ] 466/867, 7.7 task/s, elapsed: 61s, ETA:    52s\n",
            "[>>>>>>>>>>>>>>>>              ] 467/867, 7.7 task/s, elapsed: 61s, ETA:    52s\n",
            "[>>>>>>>>>>>>>>>>              ] 468/867, 7.7 task/s, elapsed: 61s, ETA:    52s\n",
            "[>>>>>>>>>>>>>>>>              ] 469/867, 7.7 task/s, elapsed: 61s, ETA:    52s\n",
            "[>>>>>>>>>>>>>>>>              ] 470/867, 7.7 task/s, elapsed: 61s, ETA:    52s\n",
            "[>>>>>>>>>>>>>>>>              ] 471/867, 7.7 task/s, elapsed: 61s, ETA:    52s\n",
            "[>>>>>>>>>>>>>>>>              ] 472/867, 7.7 task/s, elapsed: 62s, ETA:    52s\n",
            "[>>>>>>>>>>>>>>>>              ] 473/867, 7.7 task/s, elapsed: 62s, ETA:    51s\n",
            "[>>>>>>>>>>>>>>>>              ] 474/867, 7.7 task/s, elapsed: 62s, ETA:    51s\n",
            "[>>>>>>>>>>>>>>>>              ] 475/867, 7.7 task/s, elapsed: 62s, ETA:    51s\n",
            "[>>>>>>>>>>>>>>>>              ] 476/867, 7.7 task/s, elapsed: 62s, ETA:    51s\n",
            "[>>>>>>>>>>>>>>>>              ] 477/867, 7.7 task/s, elapsed: 62s, ETA:    51s\n",
            "[>>>>>>>>>>>>>>>>              ] 478/867, 7.7 task/s, elapsed: 62s, ETA:    51s\n",
            "[>>>>>>>>>>>>>>>>              ] 479/867, 7.7 task/s, elapsed: 63s, ETA:    51s\n",
            "[>>>>>>>>>>>>>>>>              ] 480/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 481/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 482/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 483/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 484/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 485/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 486/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 487/867, 7.7 task/s, elapsed: 63s, ETA:    50s\n",
            "[>>>>>>>>>>>>>>>>              ] 488/867, 7.7 task/s, elapsed: 64s, ETA:    49s\n",
            "[>>>>>>>>>>>>>>>>              ] 489/867, 7.7 task/s, elapsed: 64s, ETA:    49s\n",
            "[>>>>>>>>>>>>>>>>              ] 490/867, 7.7 task/s, elapsed: 64s, ETA:    49s\n",
            "[>>>>>>>>>>>>>>>>              ] 491/867, 7.7 task/s, elapsed: 64s, ETA:    49s\n",
            "[>>>>>>>>>>>>>>>>>             ] 492/867, 7.7 task/s, elapsed: 64s, ETA:    49s\n",
            "[>>>>>>>>>>>>>>>>>             ] 493/867, 7.7 task/s, elapsed: 64s, ETA:    49s\n",
            "[>>>>>>>>>>>>>>>>>             ] 494/867, 7.7 task/s, elapsed: 64s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 495/867, 7.7 task/s, elapsed: 64s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 496/867, 7.7 task/s, elapsed: 64s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 497/867, 7.7 task/s, elapsed: 65s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 498/867, 7.7 task/s, elapsed: 65s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 499/867, 7.7 task/s, elapsed: 65s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 500/867, 7.7 task/s, elapsed: 65s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 501/867, 7.7 task/s, elapsed: 65s, ETA:    48s\n",
            "[>>>>>>>>>>>>>>>>>             ] 502/867, 7.7 task/s, elapsed: 65s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 503/867, 7.7 task/s, elapsed: 65s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 504/867, 7.7 task/s, elapsed: 66s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 505/867, 7.7 task/s, elapsed: 66s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 506/867, 7.7 task/s, elapsed: 66s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 507/867, 7.7 task/s, elapsed: 66s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 508/867, 7.7 task/s, elapsed: 66s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 509/867, 7.7 task/s, elapsed: 66s, ETA:    47s\n",
            "[>>>>>>>>>>>>>>>>>             ] 510/867, 7.7 task/s, elapsed: 66s, ETA:    46s\n",
            "[>>>>>>>>>>>>>>>>>             ] 511/867, 7.7 task/s, elapsed: 66s, ETA:    46s\n",
            "[>>>>>>>>>>>>>>>>>             ] 512/867, 7.7 task/s, elapsed: 66s, ETA:    46s\n",
            "[>>>>>>>>>>>>>>>>>             ] 513/867, 7.7 task/s, elapsed: 67s, ETA:    46s\n",
            "[>>>>>>>>>>>>>>>>>             ] 514/867, 7.7 task/s, elapsed: 67s, ETA:    46s\n",
            "[>>>>>>>>>>>>>>>>>             ] 515/867, 7.7 task/s, elapsed: 67s, ETA:    46s\n",
            "[>>>>>>>>>>>>>>>>>             ] 516/867, 7.7 task/s, elapsed: 67s, ETA:    46s\n",
            "[>>>>>>>>>>>>>>>>>             ] 517/867, 7.7 task/s, elapsed: 67s, ETA:    45s\n",
            "[>>>>>>>>>>>>>>>>>             ] 518/867, 7.7 task/s, elapsed: 67s, ETA:    45s\n",
            "[>>>>>>>>>>>>>>>>>             ] 519/867, 7.7 task/s, elapsed: 67s, ETA:    45s\n",
            "[>>>>>>>>>>>>>>>>>             ] 520/867, 7.7 task/s, elapsed: 67s, ETA:    45s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 521/867, 7.7 task/s, elapsed: 68s, ETA:    45s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 522/867, 7.7 task/s, elapsed: 68s, ETA:    45s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 523/867, 7.7 task/s, elapsed: 68s, ETA:    45s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 524/867, 7.7 task/s, elapsed: 68s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 525/867, 7.7 task/s, elapsed: 68s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 526/867, 7.7 task/s, elapsed: 68s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 527/867, 7.7 task/s, elapsed: 68s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 528/867, 7.7 task/s, elapsed: 69s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 529/867, 7.7 task/s, elapsed: 69s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 530/867, 7.7 task/s, elapsed: 69s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 531/867, 7.7 task/s, elapsed: 69s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 532/867, 7.7 task/s, elapsed: 69s, ETA:    43s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 533/867, 7.7 task/s, elapsed: 69s, ETA:    43s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 534/867, 7.7 task/s, elapsed: 69s, ETA:    43s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 535/867, 7.7 task/s, elapsed: 69s, ETA:    43s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 536/867, 7.7 task/s, elapsed: 69s, ETA:    43s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 537/867, 7.7 task/s, elapsed: 70s, ETA:    43s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 538/867, 7.7 task/s, elapsed: 70s, ETA:    43s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 539/867, 7.7 task/s, elapsed: 70s, ETA:    42s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 540/867, 7.7 task/s, elapsed: 70s, ETA:    42s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 541/867, 7.7 task/s, elapsed: 70s, ETA:    42s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 542/867, 7.7 task/s, elapsed: 70s, ETA:    42s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 543/867, 7.7 task/s, elapsed: 70s, ETA:    42s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 544/867, 7.7 task/s, elapsed: 70s, ETA:    42s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 545/867, 7.7 task/s, elapsed: 70s, ETA:    42s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 546/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 547/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 548/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>            ] 549/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 550/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 551/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 552/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 553/867, 7.7 task/s, elapsed: 71s, ETA:    41s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 554/867, 7.7 task/s, elapsed: 72s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 555/867, 7.7 task/s, elapsed: 72s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 556/867, 7.7 task/s, elapsed: 72s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 557/867, 7.7 task/s, elapsed: 72s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 558/867, 7.7 task/s, elapsed: 72s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 559/867, 7.7 task/s, elapsed: 72s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 560/867, 7.7 task/s, elapsed: 72s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 561/867, 7.7 task/s, elapsed: 73s, ETA:    40s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 562/867, 7.7 task/s, elapsed: 73s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 563/867, 7.7 task/s, elapsed: 73s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 564/867, 7.7 task/s, elapsed: 73s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 565/867, 7.7 task/s, elapsed: 73s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 566/867, 7.7 task/s, elapsed: 73s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 567/867, 7.7 task/s, elapsed: 73s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 568/867, 7.7 task/s, elapsed: 73s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 569/867, 7.7 task/s, elapsed: 74s, ETA:    39s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 570/867, 7.7 task/s, elapsed: 74s, ETA:    38s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 571/867, 7.7 task/s, elapsed: 74s, ETA:    38s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 572/867, 7.7 task/s, elapsed: 74s, ETA:    38s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 573/867, 7.7 task/s, elapsed: 74s, ETA:    38s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 574/867, 7.7 task/s, elapsed: 74s, ETA:    38s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 575/867, 7.7 task/s, elapsed: 74s, ETA:    38s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 576/867, 7.7 task/s, elapsed: 74s, ETA:    38s\n",
            "[>>>>>>>>>>>>>>>>>>>           ] 577/867, 7.7 task/s, elapsed: 74s, ETA:    37s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 578/867, 7.8 task/s, elapsed: 75s, ETA:    37s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 579/867, 7.8 task/s, elapsed: 75s, ETA:    37s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 580/867, 7.8 task/s, elapsed: 75s, ETA:    37s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 581/867, 7.8 task/s, elapsed: 75s, ETA:    37s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 582/867, 7.8 task/s, elapsed: 75s, ETA:    37s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 583/867, 7.8 task/s, elapsed: 75s, ETA:    37s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 584/867, 7.8 task/s, elapsed: 75s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 585/867, 7.8 task/s, elapsed: 75s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 586/867, 7.8 task/s, elapsed: 76s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 587/867, 7.8 task/s, elapsed: 76s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 588/867, 7.8 task/s, elapsed: 76s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 589/867, 7.8 task/s, elapsed: 76s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 590/867, 7.8 task/s, elapsed: 76s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 591/867, 7.8 task/s, elapsed: 76s, ETA:    36s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 592/867, 7.8 task/s, elapsed: 76s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 593/867, 7.8 task/s, elapsed: 76s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 594/867, 7.8 task/s, elapsed: 77s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 595/867, 7.8 task/s, elapsed: 77s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 596/867, 7.8 task/s, elapsed: 77s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 597/867, 7.8 task/s, elapsed: 77s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 598/867, 7.8 task/s, elapsed: 77s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 599/867, 7.8 task/s, elapsed: 77s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 600/867, 7.8 task/s, elapsed: 77s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 601/867, 7.8 task/s, elapsed: 77s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 602/867, 7.8 task/s, elapsed: 78s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 603/867, 7.8 task/s, elapsed: 78s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 604/867, 7.8 task/s, elapsed: 78s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 605/867, 7.8 task/s, elapsed: 78s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>          ] 606/867, 7.7 task/s, elapsed: 78s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 607/867, 7.7 task/s, elapsed: 78s, ETA:    34s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 608/867, 7.7 task/s, elapsed: 78s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 609/867, 7.7 task/s, elapsed: 79s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 610/867, 7.7 task/s, elapsed: 79s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 611/867, 7.7 task/s, elapsed: 79s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 612/867, 7.7 task/s, elapsed: 79s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 613/867, 7.7 task/s, elapsed: 79s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 614/867, 7.7 task/s, elapsed: 79s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 615/867, 7.7 task/s, elapsed: 79s, ETA:    33s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 616/867, 7.7 task/s, elapsed: 80s, ETA:    32s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 617/867, 7.7 task/s, elapsed: 80s, ETA:    32s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 618/867, 7.7 task/s, elapsed: 80s, ETA:    32s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 619/867, 7.7 task/s, elapsed: 80s, ETA:    32s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 620/867, 7.8 task/s, elapsed: 80s, ETA:    32s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 621/867, 7.8 task/s, elapsed: 80s, ETA:    32s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 622/867, 7.8 task/s, elapsed: 80s, ETA:    32s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 623/867, 7.8 task/s, elapsed: 80s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 624/867, 7.8 task/s, elapsed: 80s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 625/867, 7.8 task/s, elapsed: 81s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 626/867, 7.8 task/s, elapsed: 81s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 627/867, 7.8 task/s, elapsed: 81s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 628/867, 7.8 task/s, elapsed: 81s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 629/867, 7.8 task/s, elapsed: 81s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 630/867, 7.8 task/s, elapsed: 81s, ETA:    31s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 631/867, 7.8 task/s, elapsed: 81s, ETA:    30s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 632/867, 7.8 task/s, elapsed: 81s, ETA:    30s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 633/867, 7.8 task/s, elapsed: 81s, ETA:    30s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 634/867, 7.8 task/s, elapsed: 82s, ETA:    30s\n",
            "[>>>>>>>>>>>>>>>>>>>>>         ] 635/867, 7.8 task/s, elapsed: 82s, ETA:    30s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 636/867, 7.8 task/s, elapsed: 82s, ETA:    30s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 637/867, 7.8 task/s, elapsed: 82s, ETA:    30s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 638/867, 7.8 task/s, elapsed: 82s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 639/867, 7.8 task/s, elapsed: 82s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 640/867, 7.8 task/s, elapsed: 82s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 641/867, 7.8 task/s, elapsed: 82s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 642/867, 7.8 task/s, elapsed: 83s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 643/867, 7.8 task/s, elapsed: 83s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 644/867, 7.8 task/s, elapsed: 83s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 645/867, 7.8 task/s, elapsed: 83s, ETA:    29s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 646/867, 7.8 task/s, elapsed: 83s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 647/867, 7.8 task/s, elapsed: 83s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 648/867, 7.8 task/s, elapsed: 83s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 649/867, 7.8 task/s, elapsed: 84s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 650/867, 7.8 task/s, elapsed: 84s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 651/867, 7.8 task/s, elapsed: 84s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 652/867, 7.8 task/s, elapsed: 84s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 653/867, 7.8 task/s, elapsed: 84s, ETA:    28s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 654/867, 7.8 task/s, elapsed: 84s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 655/867, 7.8 task/s, elapsed: 84s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 656/867, 7.8 task/s, elapsed: 85s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 657/867, 7.8 task/s, elapsed: 85s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 658/867, 7.8 task/s, elapsed: 85s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 659/867, 7.8 task/s, elapsed: 85s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 660/867, 7.8 task/s, elapsed: 85s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 661/867, 7.8 task/s, elapsed: 85s, ETA:    27s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 662/867, 7.8 task/s, elapsed: 85s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 663/867, 7.8 task/s, elapsed: 85s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>        ] 664/867, 7.8 task/s, elapsed: 85s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 665/867, 7.8 task/s, elapsed: 86s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 666/867, 7.8 task/s, elapsed: 86s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 667/867, 7.8 task/s, elapsed: 86s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 668/867, 7.8 task/s, elapsed: 86s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 669/867, 7.8 task/s, elapsed: 86s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 670/867, 7.8 task/s, elapsed: 86s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 671/867, 7.8 task/s, elapsed: 86s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 672/867, 7.8 task/s, elapsed: 86s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 673/867, 7.8 task/s, elapsed: 86s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 674/867, 7.8 task/s, elapsed: 87s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 675/867, 7.8 task/s, elapsed: 87s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 676/867, 7.8 task/s, elapsed: 87s, ETA:    25s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 677/867, 7.8 task/s, elapsed: 87s, ETA:    24s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 678/867, 7.8 task/s, elapsed: 87s, ETA:    24s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 679/867, 7.8 task/s, elapsed: 87s, ETA:    24s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 680/867, 7.8 task/s, elapsed: 87s, ETA:    24s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 681/867, 7.8 task/s, elapsed: 87s, ETA:    24s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 682/867, 7.8 task/s, elapsed: 88s, ETA:    24s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 683/867, 7.8 task/s, elapsed: 88s, ETA:    24s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 684/867, 7.8 task/s, elapsed: 88s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 685/867, 7.8 task/s, elapsed: 88s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 686/867, 7.8 task/s, elapsed: 88s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 687/867, 7.8 task/s, elapsed: 88s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 688/867, 7.8 task/s, elapsed: 88s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 689/867, 7.8 task/s, elapsed: 89s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 690/867, 7.8 task/s, elapsed: 89s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 691/867, 7.8 task/s, elapsed: 89s, ETA:    23s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 692/867, 7.8 task/s, elapsed: 89s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>       ] 693/867, 7.8 task/s, elapsed: 89s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 694/867, 7.8 task/s, elapsed: 89s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 695/867, 7.8 task/s, elapsed: 89s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 696/867, 7.8 task/s, elapsed: 89s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 697/867, 7.8 task/s, elapsed: 90s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 698/867, 7.8 task/s, elapsed: 90s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 699/867, 7.8 task/s, elapsed: 90s, ETA:    22s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 700/867, 7.8 task/s, elapsed: 90s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 701/867, 7.8 task/s, elapsed: 90s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 702/867, 7.8 task/s, elapsed: 90s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 703/867, 7.8 task/s, elapsed: 90s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 704/867, 7.8 task/s, elapsed: 90s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 705/867, 7.8 task/s, elapsed: 90s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 706/867, 7.8 task/s, elapsed: 91s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 707/867, 7.8 task/s, elapsed: 91s, ETA:    21s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 708/867, 7.8 task/s, elapsed: 91s, ETA:    20s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 709/867, 7.8 task/s, elapsed: 91s, ETA:    20s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 710/867, 7.8 task/s, elapsed: 91s, ETA:    20s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 711/867, 7.8 task/s, elapsed: 91s, ETA:    20s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 712/867, 7.8 task/s, elapsed: 91s, ETA:    20s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 713/867, 7.8 task/s, elapsed: 91s, ETA:    20s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 714/867, 7.8 task/s, elapsed: 92s, ETA:    20s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 715/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 716/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 717/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 718/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 719/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 720/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 721/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>      ] 722/867, 7.8 task/s, elapsed: 92s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 723/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 724/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 725/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 726/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 727/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 728/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 729/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 730/867, 7.8 task/s, elapsed: 93s, ETA:    18s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 731/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 732/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 733/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 734/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 735/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 736/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 737/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 738/867, 7.8 task/s, elapsed: 94s, ETA:    17s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 739/867, 7.8 task/s, elapsed: 95s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 740/867, 7.8 task/s, elapsed: 95s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 741/867, 7.8 task/s, elapsed: 95s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 742/867, 7.8 task/s, elapsed: 95s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 743/867, 7.8 task/s, elapsed: 95s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 744/867, 7.8 task/s, elapsed: 95s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 745/867, 7.8 task/s, elapsed: 95s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 746/867, 7.8 task/s, elapsed: 96s, ETA:    16s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 747/867, 7.8 task/s, elapsed: 96s, ETA:    15s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 748/867, 7.8 task/s, elapsed: 96s, ETA:    15s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 749/867, 7.8 task/s, elapsed: 96s, ETA:    15s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 750/867, 7.8 task/s, elapsed: 96s, ETA:    15s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 751/867, 7.8 task/s, elapsed: 96s, ETA:    15s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 752/867, 7.8 task/s, elapsed: 96s, ETA:    15s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 753/867, 7.8 task/s, elapsed: 97s, ETA:    15s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 754/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 755/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 756/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 757/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 758/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 759/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 760/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 761/867, 7.8 task/s, elapsed: 97s, ETA:    14s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 762/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 763/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 764/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 765/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 766/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 767/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 768/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 769/867, 7.8 task/s, elapsed: 98s, ETA:    13s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 770/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 771/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 772/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 773/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 774/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 775/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 776/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 777/867, 7.8 task/s, elapsed: 99s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 778/867, 7.8 task/s, elapsed: 100s, ETA:    11s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 779/867, 7.8 task/s, elapsed: 100s, ETA:    11s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 780/867, 7.8 task/s, elapsed: 100s, ETA:    11s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 781/867, 7.8 task/s, elapsed: 100s, ETA:    11s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 782/867, 7.8 task/s, elapsed: 100s, ETA:    11s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 783/867, 7.8 task/s, elapsed: 100s, ETA:    11s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 784/867, 7.8 task/s, elapsed: 100s, ETA:    11s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 785/867, 7.8 task/s, elapsed: 100s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 786/867, 7.8 task/s, elapsed: 101s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 787/867, 7.8 task/s, elapsed: 101s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 788/867, 7.8 task/s, elapsed: 101s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 789/867, 7.8 task/s, elapsed: 101s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 790/867, 7.8 task/s, elapsed: 101s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 791/867, 7.8 task/s, elapsed: 101s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 792/867, 7.8 task/s, elapsed: 101s, ETA:    10s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 793/867, 7.8 task/s, elapsed: 101s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 794/867, 7.8 task/s, elapsed: 101s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 795/867, 7.8 task/s, elapsed: 102s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 796/867, 7.8 task/s, elapsed: 102s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 797/867, 7.8 task/s, elapsed: 102s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 798/867, 7.8 task/s, elapsed: 102s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 799/867, 7.8 task/s, elapsed: 102s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 800/867, 7.8 task/s, elapsed: 102s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 801/867, 7.8 task/s, elapsed: 102s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 802/867, 7.8 task/s, elapsed: 103s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 803/867, 7.8 task/s, elapsed: 103s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 804/867, 7.8 task/s, elapsed: 103s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 805/867, 7.8 task/s, elapsed: 103s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 806/867, 7.8 task/s, elapsed: 103s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 807/867, 7.8 task/s, elapsed: 103s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 808/867, 7.8 task/s, elapsed: 103s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 809/867, 7.8 task/s, elapsed: 103s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 810/867, 7.8 task/s, elapsed: 103s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 811/867, 7.8 task/s, elapsed: 104s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 812/867, 7.8 task/s, elapsed: 104s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 813/867, 7.8 task/s, elapsed: 104s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 814/867, 7.8 task/s, elapsed: 104s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 815/867, 7.8 task/s, elapsed: 104s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 816/867, 7.8 task/s, elapsed: 104s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 817/867, 7.8 task/s, elapsed: 104s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 818/867, 7.8 task/s, elapsed: 104s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 819/867, 7.8 task/s, elapsed: 105s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 820/867, 7.8 task/s, elapsed: 105s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 821/867, 7.8 task/s, elapsed: 105s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 822/867, 7.8 task/s, elapsed: 105s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 823/867, 7.8 task/s, elapsed: 105s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 824/867, 7.8 task/s, elapsed: 105s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 825/867, 7.8 task/s, elapsed: 105s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 826/867, 7.8 task/s, elapsed: 105s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 827/867, 7.8 task/s, elapsed: 106s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 828/867, 7.8 task/s, elapsed: 106s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 829/867, 7.8 task/s, elapsed: 106s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 830/867, 7.8 task/s, elapsed: 106s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 831/867, 7.8 task/s, elapsed: 106s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 832/867, 7.8 task/s, elapsed: 106s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 833/867, 7.8 task/s, elapsed: 106s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 834/867, 7.8 task/s, elapsed: 107s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 835/867, 7.8 task/s, elapsed: 107s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 836/867, 7.8 task/s, elapsed: 107s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 837/867, 7.8 task/s, elapsed: 107s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 838/867, 7.8 task/s, elapsed: 107s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 839/867, 7.8 task/s, elapsed: 107s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 840/867, 7.8 task/s, elapsed: 107s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 841/867, 7.8 task/s, elapsed: 107s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 842/867, 7.8 task/s, elapsed: 107s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 843/867, 7.8 task/s, elapsed: 108s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 844/867, 7.8 task/s, elapsed: 108s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 845/867, 7.8 task/s, elapsed: 108s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 846/867, 7.8 task/s, elapsed: 108s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 847/867, 7.8 task/s, elapsed: 108s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 848/867, 7.8 task/s, elapsed: 108s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 849/867, 7.8 task/s, elapsed: 108s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 850/867, 7.8 task/s, elapsed: 108s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 851/867, 7.8 task/s, elapsed: 108s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 852/867, 7.8 task/s, elapsed: 109s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 853/867, 7.8 task/s, elapsed: 109s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 854/867, 7.8 task/s, elapsed: 109s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 855/867, 7.8 task/s, elapsed: 109s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 856/867, 7.8 task/s, elapsed: 109s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 857/867, 7.8 task/s, elapsed: 109s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 858/867, 7.8 task/s, elapsed: 109s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 859/867, 7.8 task/s, elapsed: 110s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 860/867, 7.8 task/s, elapsed: 110s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 861/867, 7.8 task/s, elapsed: 110s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 862/867, 7.8 task/s, elapsed: 110s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 863/867, 7.8 task/s, elapsed: 110s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 864/867, 7.8 task/s, elapsed: 110s, ETA:     0s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 865/867, 7.8 task/s, elapsed: 110s, ETA:     0s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 866/867, 7.8 task/s, elapsed: 110s, ETA:     0s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 867/867, 7.8 task/s, elapsed: 111s, ETA:     0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:757: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:828: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "! python demo/video_demo.py demo/houhai/video/rip_01.mp4 \\\n",
        "                                configs/my/yolov5_0705.py \\\n",
        "                                model/yolov5_0705.pth \\\n",
        "                                --out demo/vis/houhai-res/yolov5_0705/0.05/video/rip_01.mp4 \\\n",
        "                                --score-thr 0.05 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: model/yolov5_0705.pth\n",
            "03/17 20:26:16 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/25, elapsed: 0s, ETA:03/17 20:26:16 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "03/17 20:26:16 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>                                 ] 1/25, 0.3 task/s, elapsed: 3s, ETA:    82s\n",
            "[>>                                ] 2/25, 0.6 task/s, elapsed: 4s, ETA:    42s\n",
            "[>>>>                              ] 3/25, 0.8 task/s, elapsed: 4s, ETA:    28s\n",
            "[>>>>>                             ] 4/25, 1.0 task/s, elapsed: 4s, ETA:    21s\n",
            "[>>>>>>                            ] 5/25, 1.2 task/s, elapsed: 4s, ETA:    16s\n",
            "[>>>>>>>>                          ] 6/25, 1.4 task/s, elapsed: 4s, ETA:    13s\n",
            "[>>>>>>>>>                         ] 7/25, 1.6 task/s, elapsed: 4s, ETA:    11s\n",
            "[>>>>>>>>>>                        ] 8/25, 1.7 task/s, elapsed: 5s, ETA:    10s\n",
            "[>>>>>>>>>>>>                      ] 9/25, 1.9 task/s, elapsed: 5s, ETA:     8s\n",
            "[>>>>>>>>>>>>>                    ] 10/25, 2.0 task/s, elapsed: 5s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>                   ] 11/25, 2.2 task/s, elapsed: 5s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>                  ] 12/25, 2.3 task/s, elapsed: 5s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>                ] 13/25, 2.4 task/s, elapsed: 5s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>               ] 14/25, 2.5 task/s, elapsed: 6s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>              ] 15/25, 2.6 task/s, elapsed: 6s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>            ] 16/25, 2.7 task/s, elapsed: 6s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>           ] 17/25, 2.8 task/s, elapsed: 6s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>          ] 18/25, 2.9 task/s, elapsed: 6s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>        ] 19/25, 3.0 task/s, elapsed: 6s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>       ] 20/25, 3.1 task/s, elapsed: 6s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 21/25, 3.2 task/s, elapsed: 7s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 22/25, 3.2 task/s, elapsed: 7s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 23/25, 3.3 task/s, elapsed: 7s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 24/25, 3.3 task/s, elapsed: 7s, ETA:     0s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 25/25, 3.4 task/s, elapsed: 7s, ETA:     0s\n",
            "Results have been saved at d:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\gaolong-res\\yolov5_0705\\0.05\\02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:757: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:828: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "! python demo/image_demo.py demo/gaolong/02 \\\n",
        "                                configs/my/yolov5_0705.py \\\n",
        "                                model/yolov5_0705.pth \\\n",
        "                                --out-dir demo/vis/gaolong-res/yolov5_0705\\0.02\\02 \\\n",
        "                                --score-thr 0.02 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: model/yolov5_0705.pth\n",
            "11/08 09:12:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/28, elapsed: 0s, ETA:11/08 09:12:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "11/08 09:12:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>                                 ] 1/28, 0.2 task/s, elapsed: 4s, ETA:   111s\n",
            "[>>                                ] 2/28, 0.5 task/s, elapsed: 4s, ETA:    56s\n",
            "[>>>                               ] 3/28, 0.7 task/s, elapsed: 4s, ETA:    37s\n",
            "[>>>>                              ] 4/28, 0.9 task/s, elapsed: 5s, ETA:    28s\n",
            "[>>>>>>                            ] 5/28, 1.0 task/s, elapsed: 5s, ETA:    22s\n",
            "[>>>>>>>                           ] 6/28, 1.2 task/s, elapsed: 5s, ETA:    18s\n",
            "[>>>>>>>>                          ] 7/28, 1.4 task/s, elapsed: 5s, ETA:    16s\n",
            "[>>>>>>>>>                         ] 8/28, 1.5 task/s, elapsed: 5s, ETA:    13s\n",
            "[>>>>>>>>>>                        ] 9/28, 1.6 task/s, elapsed: 6s, ETA:    12s\n",
            "[>>>>>>>>>>>                      ] 10/28, 1.7 task/s, elapsed: 6s, ETA:    10s\n",
            "[>>>>>>>>>>>>                     ] 11/28, 1.8 task/s, elapsed: 6s, ETA:     9s\n",
            "[>>>>>>>>>>>>>>                   ] 12/28, 2.0 task/s, elapsed: 6s, ETA:     8s\n",
            "[>>>>>>>>>>>>>>>                  ] 13/28, 2.1 task/s, elapsed: 6s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>                 ] 14/28, 2.1 task/s, elapsed: 7s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>>>>                ] 15/28, 2.2 task/s, elapsed: 7s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>               ] 16/28, 2.3 task/s, elapsed: 7s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>             ] 17/28, 2.4 task/s, elapsed: 7s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>>>>            ] 18/28, 2.5 task/s, elapsed: 7s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>           ] 19/28, 2.6 task/s, elapsed: 7s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>          ] 20/28, 2.6 task/s, elapsed: 8s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>         ] 21/28, 2.7 task/s, elapsed: 8s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>        ] 22/28, 2.7 task/s, elapsed: 8s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 23/28, 2.8 task/s, elapsed: 8s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 24/28, 2.8 task/s, elapsed: 8s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 25/28, 2.9 task/s, elapsed: 9s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 26/28, 2.9 task/s, elapsed: 9s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 27/28, 2.9 task/s, elapsed: 9s, ETA:     0s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 28/28, 3.0 task/s, elapsed: 9s, ETA:     0s\n",
            "Results have been saved at d:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\sansha-res\\yolov5_0705\\0.05\\03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:757: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:828: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "! python demo/image_demo.py demo/sansha/01 \\\n",
        "                                configs/my/yolov5_0705.py \\\n",
        "                                model/yolov5_0705.pth \\\n",
        "                                --out-dir demo/vis/sansha-res/yolov5_0705\\0.05\\03 \\\n",
        "                                --score-thr 0.05 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python demo/image_demo.py demo/sansha/01 \\\n",
        "                                configs/my/yolov8_swin_rip.py \\\n",
        "                                model/best_coco_bbox_mAP_epoch_295_yolov8_swin_bs8.pth \\\n",
        "                                --out-dir demo/vis/sansha-res/yolov8_swin_rip_e295 \\\n",
        "                                --score-thr 0.03 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loads checkpoint by local backend from path: model/best_coco_bbox_mAP_epoch_295_yolov8_swin_bs8.pth\n",
            "11/06 11:10:30 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/28, elapsed: 0s, ETA:11/06 11:10:30 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "11/06 11:10:30 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>                                 ] 1/28, 0.5 task/s, elapsed: 2s, ETA:    55s\n",
            "[>>                                ] 2/28, 0.9 task/s, elapsed: 2s, ETA:    30s\n",
            "[>>>                               ] 3/28, 1.2 task/s, elapsed: 3s, ETA:    21s\n",
            "[>>>>                              ] 4/28, 1.4 task/s, elapsed: 3s, ETA:    17s\n",
            "[>>>>>>                            ] 5/28, 1.6 task/s, elapsed: 3s, ETA:    14s\n",
            "[>>>>>>>                           ] 6/28, 1.8 task/s, elapsed: 3s, ETA:    12s\n",
            "[>>>>>>>>                          ] 7/28, 2.0 task/s, elapsed: 4s, ETA:    11s\n",
            "[>>>>>>>>>                         ] 8/28, 2.1 task/s, elapsed: 4s, ETA:     9s\n",
            "[>>>>>>>>>>                        ] 9/28, 2.2 task/s, elapsed: 4s, ETA:     9s\n",
            "[>>>>>>>>>>>                      ] 10/28, 2.3 task/s, elapsed: 4s, ETA:     8s\n",
            "[>>>>>>>>>>>>                     ] 11/28, 2.4 task/s, elapsed: 5s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>                   ] 12/28, 2.5 task/s, elapsed: 5s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>                  ] 13/28, 2.6 task/s, elapsed: 5s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>                 ] 14/28, 2.6 task/s, elapsed: 5s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>                ] 15/28, 2.7 task/s, elapsed: 6s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>               ] 16/28, 2.7 task/s, elapsed: 6s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>             ] 17/28, 2.8 task/s, elapsed: 6s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>            ] 18/28, 2.8 task/s, elapsed: 6s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>           ] 19/28, 2.9 task/s, elapsed: 7s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>          ] 20/28, 2.9 task/s, elapsed: 7s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>         ] 21/28, 2.9 task/s, elapsed: 7s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>        ] 22/28, 3.0 task/s, elapsed: 7s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 23/28, 3.0 task/s, elapsed: 8s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 24/28, 3.0 task/s, elapsed: 8s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 25/28, 3.1 task/s, elapsed: 8s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 26/28, 3.1 task/s, elapsed: 8s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 27/28, 3.1 task/s, elapsed: 9s, ETA:     0s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 28/28, 3.1 task/s, elapsed: 9s, ETA:     0s\n",
            "Results have been saved at d:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\sansha-res\\yolov8_swin_rip_e295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:757: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:828: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "! python demo/image_demo.py demo/houhai/03 \\\n",
        "                                configs/my/yolov8_swin_rip.py \\\n",
        "                                model/best_coco_bbox_mAP_epoch_295_yolov8_swin_bs8.pth \\\n",
        "                                --out-dir demo/vis/houhai-res/yolov8_swin_rip_e295 \\\n",
        "                                --score-thr 0.5 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: model/best_coco_bbox_mAP_epoch_200_yolov5_swin_bs8.pth\n",
            "10/22 21:05:16 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/8, elapsed: 0s, ETA:10/22 21:05:16 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "10/22 21:05:16 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>>>>                               ] 1/8, 0.5 task/s, elapsed: 2s, ETA:    15s\n",
            "[>>>>>>>>                           ] 2/8, 0.8 task/s, elapsed: 2s, ETA:     7s\n",
            "[>>>>>>>>>>>>>                      ] 3/8, 1.1 task/s, elapsed: 3s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>                  ] 4/8, 1.3 task/s, elapsed: 3s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>              ] 5/8, 1.5 task/s, elapsed: 3s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>         ] 6/8, 1.7 task/s, elapsed: 3s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 7/8, 1.9 task/s, elapsed: 4s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 8/8, 2.0 task/s, elapsed: 4s, ETA:     0s\n",
            "Results have been saved at d:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\houhai-res\\yolov5_swin_rip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:757: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:828: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "! python demo/image_demo.py demo/houhai/03 \\\n",
        "                                configs/my/yolov5_swin_rip.py \\\n",
        "                                model/best_coco_bbox_mAP_epoch_200_yolov5_swin_bs8.pth \\\n",
        "                                --out-dir demo/vis/houhai-res/yolov5_swin_rip \\\n",
        "                                --score-thr 0.03 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: model/best_coco_bbox_mAP_epoch_200_yolov5_swin_bs8.pth\n",
            "11/06 11:12:39 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/28, elapsed: 0s, ETA:11/06 11:12:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "11/06 11:12:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>                                 ] 1/28, 0.5 task/s, elapsed: 2s, ETA:    56s\n",
            "[>>                                ] 2/28, 0.9 task/s, elapsed: 2s, ETA:    30s\n",
            "[>>>                               ] 3/28, 1.2 task/s, elapsed: 3s, ETA:    21s\n",
            "[>>>>                              ] 4/28, 1.4 task/s, elapsed: 3s, ETA:    17s\n",
            "[>>>>>>                            ] 5/28, 1.6 task/s, elapsed: 3s, ETA:    14s\n",
            "[>>>>>>>                           ] 6/28, 1.8 task/s, elapsed: 3s, ETA:    12s\n",
            "[>>>>>>>>                          ] 7/28, 2.0 task/s, elapsed: 4s, ETA:    11s\n",
            "[>>>>>>>>>                         ] 8/28, 2.1 task/s, elapsed: 4s, ETA:     9s\n",
            "[>>>>>>>>>>                        ] 9/28, 2.3 task/s, elapsed: 4s, ETA:     8s\n",
            "[>>>>>>>>>>>                      ] 10/28, 2.3 task/s, elapsed: 4s, ETA:     8s\n",
            "[>>>>>>>>>>>>                     ] 11/28, 2.4 task/s, elapsed: 5s, ETA:     7s\n",
            "[>>>>>>>>>>>>>>                   ] 12/28, 2.5 task/s, elapsed: 5s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>                  ] 13/28, 2.6 task/s, elapsed: 5s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>                 ] 14/28, 2.7 task/s, elapsed: 5s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>                ] 15/28, 2.7 task/s, elapsed: 5s, ETA:     5s\n",
            "[>>>>>>>>>>>>>>>>>>               ] 16/28, 2.8 task/s, elapsed: 6s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>             ] 17/28, 2.9 task/s, elapsed: 6s, ETA:     4s\n",
            "[>>>>>>>>>>>>>>>>>>>>>            ] 18/28, 2.9 task/s, elapsed: 6s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>           ] 19/28, 3.0 task/s, elapsed: 6s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>          ] 20/28, 3.0 task/s, elapsed: 7s, ETA:     3s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>         ] 21/28, 3.0 task/s, elapsed: 7s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>        ] 22/28, 3.1 task/s, elapsed: 7s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 23/28, 3.1 task/s, elapsed: 7s, ETA:     2s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 24/28, 3.1 task/s, elapsed: 8s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 25/28, 3.2 task/s, elapsed: 8s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 26/28, 3.2 task/s, elapsed: 8s, ETA:     1s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 27/28, 3.2 task/s, elapsed: 8s, ETA:     0s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 28/28, 3.3 task/s, elapsed: 9s, ETA:     0s\n",
            "Results have been saved at d:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\sansha-res\\yolov5_swin_rip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:757: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:828: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "! python demo/image_demo.py demo/sansha/01 \\\n",
        "                                configs/my/yolov5_swin_rip.py \\\n",
        "                                model/best_coco_bbox_mAP_epoch_200_yolov5_swin_bs8.pth \\\n",
        "                                --out-dir demo/vis/sansha-res/yolov5_swin_rip \\\n",
        "                                --score-thr 0.03 \\\n",
        "                                --device cuda:0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "这是在视频样例上进行推理的脚本。  \n",
        "python demo/video_demo.py \\  \n",
        "    ${VIDEO_FILE} \\  \n",
        "    ${CONFIG_FILE} \\  \n",
        "    ${CHECKPOINT_FILE} \\  \n",
        "    [--device ${GPU_ID}] \\  \n",
        "    [--score-thr ${SCORE_THR}] \\  \n",
        "    [--out ${OUT_FILE}] \\  \n",
        "    [--show] \\  \n",
        "    [--wait-time ${WAIT_TIME}]  \n",
        "  \n",
        "运行样例：  \n",
        "python demo/video_demo.py demo/demo.mp4 \\  \n",
        "    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\  \n",
        "    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\  \n",
        "    --out result.mp4  \n",
        "视频样例，显卡加速版本  \n",
        "这是在视频样例上进行推理的脚本，使用显卡加速。  \n",
        "\n",
        "python demo/video_gpuaccel_demo.py \\  \n",
        "     ${VIDEO_FILE} \\  \n",
        "     ${CONFIG_FILE} \\  \n",
        "     ${CHECKPOINT_FILE} \\  \n",
        "     [--device ${GPU_ID}] \\  \n",
        "     [--score-thr ${SCORE_THR}] \\  \n",
        "     [--nvdecode] \\  \n",
        "     [--out ${OUT_FILE}] \\  \n",
        "     [--show] \\  \n",
        "     [--wait-time ${WAIT_TIME}]  \n",
        "运行样例：  \n",
        "python demo/video_gpuaccel_demo.py demo/demo.mp4 \\  \n",
        "    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\  \n",
        "    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\  \n",
        "    --nvdecode --out result.mp4  \n",
        "\n",
        "8 GPU  \n",
        "./tools/dist_test.sh configs/glip/glip_atss_swin-t_fpn_dyhead_pretrain_obj365.py glip_tiny_a_mmdet-b3654169.pth 8\n",
        "\n",
        "tools/dist_test.sh 也支持多节点测试，不过需要依赖 PyTorch 的 启动工具  \n",
        "\n",
        "如果你的数据集格式是 VOC 或者 Cityscapes，你可以使用 tools/dataset_converters 内的脚本直接将其转化成 COCO 格式。如果是其他格式，可以使用 images2coco 脚本 进行转换。  \n",
        "python tools/dataset_converters/images2coco.py \\  \n",
        "    ${IMG_PATH} \\  \n",
        "    ${CLASSES} \\  \n",
        "    ${OUT} \\  \n",
        "    [--exclude-extensions]  \n",
        "参数：  \n",
        "IMG_PATH: 图片根路径。  \n",
        "CLASSES: 类列表文本文件名。文本中每一行存储一个类别。  \n",
        "OUT: 输出 json 文件名。 默认保存目录和 IMG_PATH 在同一级。  \n",
        "exclude-extensions: 待排除的文件后缀名。  \n",
        "在转换完成后，使用如下命令进行测试  \n",
        "  单 GPU 测试    \n",
        "python tools/test.py \\  \n",
        "    ${CONFIG_FILE} \\  \n",
        "    ${CHECKPOINT_FILE} \\  \n",
        "    [--show]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: model/best_coco_bbox_mAP_epoch_200_yolov5_swin_bs8.pth\n",
            "09/06 17:26:23 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n",
            "[                                                  ] 0/11, elapsed: 0s, ETA:09/06 17:26:23 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "09/06 17:26:23 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "\n",
            "[>>>                              ] 1/11, 0.1 task/s, elapsed: 16s, ETA:   160s\n",
            "[>>>>>>                           ] 2/11, 0.1 task/s, elapsed: 21s, ETA:    96s\n",
            "[>>>>>>>>>                        ] 3/11, 0.1 task/s, elapsed: 27s, ETA:    71s\n",
            "[>>>>>>>>>>>>                     ] 4/11, 0.1 task/s, elapsed: 31s, ETA:    55s\n",
            "[>>>>>>>>>>>>>>>                  ] 5/11, 0.1 task/s, elapsed: 37s, ETA:    44s\n",
            "[>>>>>>>>>>>>>>>>>>               ] 6/11, 0.1 task/s, elapsed: 41s, ETA:    35s\n",
            "[>>>>>>>>>>>>>>>>>>>>>            ] 7/11, 0.2 task/s, elapsed: 46s, ETA:    26s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>         ] 8/11, 0.2 task/s, elapsed: 51s, ETA:    19s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 9/11, 0.2 task/s, elapsed: 56s, ETA:    12s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 10/11, 0.2 task/s, elapsed: 60s, ETA:     6s\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 11/11, 0.2 task/s, elapsed: 65s, ETA:     0sAll done!\n",
            "Results have been saved at d:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\houhai-featmap\\yolov5_swin_rip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:984: UserWarning: Since the spatial dimensions of overlaid_image: (922, 978) and featmap: torch.Size([80, 80]) are not same, the feature map will be interpolated. This may cause mismatch problems ！\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:984: UserWarning: Since the spatial dimensions of overlaid_image: (922, 978) and featmap: torch.Size([40, 40]) are not same, the feature map will be interpolated. This may cause mismatch problems ！\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:984: UserWarning: Since the spatial dimensions of overlaid_image: (922, 978) and featmap: torch.Size([20, 20]) are not same, the feature map will be interpolated. This may cause mismatch problems ！\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:984: UserWarning: Since the spatial dimensions of overlaid_image: (896, 956) and featmap: torch.Size([80, 80]) are not same, the feature map will be interpolated. This may cause mismatch problems ！\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:984: UserWarning: Since the spatial dimensions of overlaid_image: (896, 956) and featmap: torch.Size([40, 40]) are not same, the feature map will be interpolated. This may cause mismatch problems ！\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\mmengine\\visualization\\visualizer.py:984: UserWarning: Since the spatial dimensions of overlaid_image: (896, 956) and featmap: torch.Size([20, 20]) are not same, the feature map will be interpolated. This may cause mismatch problems ！\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "! python demo/featmap_vis_demo.py demo/houhai/01 \\\n",
        "                                configs/my/yolov5_swin_rip.py \\\n",
        "                                model/best_coco_bbox_mAP_epoch_200_yolov5_swin_bs8.pth  \\\n",
        "                                --target-layers backbone \\\n",
        "                                --channel-reduction select_max \\\n",
        "                                --out-dir demo/vis/houhai-featmap/yolov5_swin_rip\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 可视化特征图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python demo/featmap_vis_demo.py demo/houhai \\\n",
        "                                configs/yolov8/yolov8_s_swin_t-v61_1xb2-1e_coco128.py \\\n",
        "                                model/swin_tiny_patch4_window7_224.pth \\\n",
        "                                --target-layers backbone \\\n",
        "                                --channel-reduction select_max \\\n",
        "                                --out-dir demo/vis/houhai-featmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 多 Target Layer\n",
        "# 可视化 backbone 输出的 backbone.stage4 和 backbone.stage3 2 个层的平均激活\n",
        "\n",
        "# @staticmethod\n",
        "# def draw_featmap(featmap: torch.Tensor, # 输入格式要求为 CHW\n",
        "#                  overlaid_image: Optional[np.ndarray] = None, # 如果同时输入了 image 数据，则特征图会叠加到 image 上绘制\n",
        "#                  channel_reduction: Optional[str] = 'squeeze_mean', # 多个通道压缩为单通道的策略\n",
        "#                  topk: int = 10, # 可选择激活度最高的 topk 个特征图显示\n",
        "#                  arrangement: Tuple[int, int] = (5, 2), # 多通道展开为多张图时候布局\n",
        "#                  resize_shape：Optional[tuple] = None, # 可以指定 resize_shape 参数来缩放特征图\n",
        "#                  alpha: float = 0.5) -> np.ndarray: # 图片和特征图绘制的叠加比例\n",
        "\n",
        "# 案例1.最大激活### 案例 1：最大激活\n",
        "# 可视化 backbone 输出的 3 个层的最大激活层\n",
        "\n",
        "! python demo/featmap_vis_demo.py demo/dog.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py \\\n",
        "                                demo/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth \\\n",
        "                                --target-layers backbone \\\n",
        "                                --channel-reduction select_max \\\n",
        "                                --out-file 'a.jpg'\n",
        "\n",
        "# ### 案例 2：平均激活\n",
        "# 可视化 neck 输出的 3 个层的所有输出特征图的平均激活\n",
        "! python demo/featmap_vis_demo.py demo/dog.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py \\\n",
        "                                demo/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth \\\n",
        "                                --target-layers neck \\\n",
        "                                --channel-reduction squeeze_mean \\\n",
        "                                --out-file 'b.jpg'\n",
        "\n",
        "# ### 案例 3：多 target layer\n",
        "# 可视化 backbone 输出的 backbone.stage4 和 backbone.stage3 2 个层的平均激活\n",
        "! python demo/featmap_vis_demo.py demo/dog.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py \\\n",
        "                                demo/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth \\\n",
        "                                --target-layers backbone.stage4 backbone.stage3 \\\n",
        "                                --channel-reduction squeeze_mean \\\n",
        "                                --out-file 'c.jpg'\n",
        "\n",
        "\n",
        "# ### 案例 4：布局重排\n",
        "# 可视化 backbone 输出的 backbone.stage4层的 topk 激活层，利用 --topk 4 --arrangement 2 2 参数选择多通道特征图中激活度最高的 3 个通道并采用 2x2 布局显示\n",
        "! python demo/featmap_vis_demo.py demo/dog.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py \\\n",
        "                                yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth \\\n",
        "                                --target-layers backbone.stage4 \\\n",
        "                                --channel-reduction None \\\n",
        "                                --topk 4 \\\n",
        "                                --arrangement 2 2 \\\n",
        "                                --out-file 'd.jpg'\n",
        "\n",
        "\n",
        "# ### 案例 5：打印网络结构\n",
        "# 不清楚网络结构，可以打印出来，然后自己写\n",
        "! python demo/featmap_vis_demo.py demo/dog.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py \\\n",
        "                                yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth \\\n",
        "                                --preview-model \n",
        "                                --method ablationcam"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grad-Based CAM 可视化\n",
        "分析特征层 bbox 级别的 Grad CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (a) 查看 neck 输出的最小输出特征图的 Grad CAM\n",
        "python demo/boxam_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                                work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                                --target-layer neck.out_layers[2]\n",
        "\n",
        "# (b) 查看 neck 输出的中等输出特征图的 Grad CAM\n",
        "python demo/boxam_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                                work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                                --target-layer neck.out_layers[1]\n",
        "\n",
        "# (c) 查看 neck 输出的最大输出特征图的 Grad CAM\n",
        "python demo/boxam_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                                work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                                --target-layer neck.out_layers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 自定义网络模块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RSP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.serialization import load\n",
        "import torchvision.models as models\n",
        "\n",
        "import imageio\n",
        "from imageio import imread\n",
        " \n",
        "# pretrained=True使用预训练的模型\n",
        "# resnet18 = models.resnet18(pretrained=True)#创建实例，模型下载.Pth文件\n",
        "swin = models.swin_v2_b(pretrained=True)#创建实例，模型下载.Pth文件\n",
        "# swin = models.swin_v2_b(pretrained=False)#创建实例，模型下载.Pth文件\n",
        "# model_path = r'D:\\Codes\\Python\\AI\\6_Model\\orcn-swin-t-dota-latest.pth'\n",
        "# model_data = torch.load(model_path)\n",
        "# swin.load_state_dict(model_data)\n",
        "swin.eval()# 切换到评估模式,使得模型BN层等失效\n",
        "img = torch.randn(1, 3, 640, 640)\n",
        "img_arr = imageio.imread(os.path.join(r'D:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\houhai', '201912.png')) #imread读入为H*W*C\n",
        "img_t = torch.from_numpy(img_arr)\n",
        "img_t = img_t.permute(2, 0, 1) #交换维度\n",
        "img_t = img_t[:3]  #只保留前3个通道\n",
        "print(img_arr)\n",
        "with torch.no_grad():\n",
        "     encoder_output = swin(img_t)\n",
        "\n",
        "print(img_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5836\\2642991316.py:11: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  img_arr = imageio.imread(os.path.join(r'D:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\houhai', '201912.png')) #imread读入为H*W*C\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'dict' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# print(img_arr)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 17\u001b[0m      encoder_output \u001b[39m=\u001b[39m net(img_t)\n\u001b[0;32m     19\u001b[0m \u001b[39m# print(img_arr)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(encoder_output)\n",
            "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.serialization import load\n",
        "import torchvision.models as models\n",
        "\n",
        "import imageio\n",
        "from imageio import imread\n",
        "\n",
        "model_pth = r'D:\\Codes\\Python\\AI\\6_Model\\orcn-swin-t-dota-latest.pth'\n",
        "net = torch.load(model_pth, map_location=torch.device('cpu'))\n",
        "img = torch.randn(1, 3, 640, 640)\n",
        "img_arr = imageio.imread(os.path.join(r'D:\\Codes\\Python\\AI\\4_Baseline\\openmmlab\\mmyolo-0.5.0\\demo\\vis\\houhai', '201912.png')) #imread读入为H*W*C\n",
        "img_t = torch.from_numpy(img_arr)\n",
        "img_t = img_t.permute(2, 0, 1) #交换维度\n",
        "img_t = img_t[:3]  #只保留前3个通道\n",
        "# print(img_arr)\n",
        "with torch.no_grad():\n",
        "     encoder_output = net(img_t)\n",
        "\n",
        "# print(img_arr)\n",
        "print(encoder_output)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 模型推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mmcv\n",
        "from mmcv.transforms import Compose\n",
        "from mmengine.utils import track_iter_progress\n",
        "from mmdet.registry import VISUALIZERS\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "# 指定模型的配置文件和 checkpoint 文件路径\n",
        "config_file = 'configs/rtmdet/rtmdet_l_8xb32-300e_coco.py'\n",
        "checkpoint_file = 'checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth'\n",
        "# 根据配置文件和 checkpoint 文件构建模型\n",
        "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
        "# 初始化可视化工具\n",
        "visualizer = VISUALIZERS.build(model.cfg.visualizer)\n",
        "# 从 checkpoint 中加载 Dataset_meta，并将其传递给模型的 init_detector\n",
        "visualizer.dataset_meta = model.dataset_meta\n",
        "# 测试单张图片并展示结果\n",
        "img = 'test.jpg'  # 或者 img = mmcv.imread(img)，这样图片仅会被读一次\n",
        "result = inference_detector(model, img)\n",
        "# 显示结果\n",
        "img = mmcv.imread(img)\n",
        "img = mmcv.imconvert(img, 'bgr', 'rgb')\n",
        "visualizer.add_datasample(\n",
        "    'result',\n",
        "    img,\n",
        "    data_sample=result,\n",
        "    draw_gt=False,\n",
        "    show=True)\n",
        "# 测试视频并展示结果\n",
        "# 构建测试 pipeline\n",
        "model.cfg.test_dataloader.dataset.pipeline[0].type = 'LoadImageFromNDArray'\n",
        "test_pipeline = Compose(model.cfg.test_dataloader.dataset.pipeline)\n",
        "# 可视化工具在第33行和35行已经初完成了初始化，如果直接在一个 jupyter nodebook 中运行这个 demo，\n",
        "# 这里则不需要再创建一个可视化工具了。\n",
        "# 初始化可视化工具\n",
        "visualizer = VISUALIZERS.build(model.cfg.visualizer)\n",
        "# 从 checkpoint 中加载 Dataset_meta，并将其传递给模型的 init_detector\n",
        "visualizer.dataset_meta = model.dataset_meta\n",
        "# 显示间隔 (ms), 0 表示暂停\n",
        "wait_time = 1\n",
        "video = mmcv.VideoReader('video.mp4')\n",
        "cv2.namedWindow('video', 0)\n",
        "for frame in track_iter_progress(video_reader):\n",
        "    result = inference_detector(model, frame, test_pipeline=test_pipeline)\n",
        "    visualizer.add_datasample(\n",
        "        name='video',\n",
        "        image=frame,\n",
        "        data_sample=result,\n",
        "        draw_gt=False,\n",
        "        show=False)\n",
        "    frame = visualizer.get_image()\n",
        "    mmcv.imshow(frame, 'video', wait_time)\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "vscode": {
      "interpreter": {
        "hash": "f28104d83f364164a14023df7a8da16cfc0355892cfe84fa9cb246c3ca2d1159"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
